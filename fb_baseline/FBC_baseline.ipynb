{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b51e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Доступные ресурсы\n",
    "import multiprocessing\n",
    "import torch\n",
    "from psutil import virtual_memory\n",
    "\n",
    "ram_gb = round(virtual_memory().total / 1024**3, 1)\n",
    "\n",
    "print('CPU:', multiprocessing.cpu_count())\n",
    "print('RAM GB:', ram_gb)\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device.type)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2907554",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/sberbank-ai/fusion_brain_aij2021.git\n",
    "# pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install tpu_star==0.0.1rc10\n",
    "!pip install albumentations==0.5.2\n",
    "!pip install einops==0.3.2 \n",
    "!pip install transformers==4.10.0 \n",
    "!pip install colorednoise==1.1.1\n",
    "!pip install catalyst==21.8 \n",
    "!pip install opencv-python==4.5.3\n",
    "!pip install gdown==4.0.2\n",
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33372983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tpu_star.experiment import TorchGPUExperiment\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'fusion_brain_aij2021/fb_baseline')\n",
    "from fb_utils.download import download_and_extract\n",
    "from fb_utils.loss import LabelSmoothing, onehot\n",
    "from fb_utils.metrics import cer, wer, string_accuracy, acc, vqa_evaluate, detection_evaluate\n",
    "from fb_utils.handwritten import simple_detect_lang, CTCLabeling, resize_if_need, make_img_padding\n",
    "from fb_utils.c2c_eval import Beam, eval_bleu\n",
    "from fb_utils.detection_vqa import (vqa_evaluation, detection_evaluation,\n",
    "                                              CrossAttentionLayer, MLP, FeedForwardComponent,\n",
    "                                              Resnet50Backbone, DetectionCriterion)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9cd44c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract('.', 'handwritten')\n",
    "download_and_extract('.', 'detection')\n",
    "download_and_extract('.', 'vqa')\n",
    "download_and_extract('.', 'c2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1645ab7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>modality</th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "      <th>stage</th>\n",
       "      <th>input_image</th>\n",
       "      <th>output_boxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trans</td>\n",
       "      <td>code</td>\n",
       "      <td>class GFG { static int countChars ( String str...</td>\n",
       "      <td>def countChars ( string , n ) : NEW_LINE INDEN...</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>a</td>\n",
       "      <td>train</td>\n",
       "      <td>38731.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>heads</td>\n",
       "      <td>train</td>\n",
       "      <td>6466.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>trans</td>\n",
       "      <td>code</td>\n",
       "      <td>import java . util . * ; public class VasyasCa...</td>\n",
       "      <td>def main ( ) : d = int ( input ( ) ) n = int (...</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>trans</td>\n",
       "      <td>code</td>\n",
       "      <td>import java . io . * ; import java . util . * ...</td>\n",
       "      <td>n , m , k = map ( int , input ( ) . split ( ) ...</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trans</td>\n",
       "      <td>code</td>\n",
       "      <td>import java . util . * ;   public class Main {...</td>\n",
       "      <td>n , m = map ( int , input ( ) . split ( ) ) fr...</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>their</td>\n",
       "      <td>train</td>\n",
       "      <td>9665.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "      <td>29347.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>palely</td>\n",
       "      <td>train</td>\n",
       "      <td>27235.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>с</td>\n",
       "      <td>valid</td>\n",
       "      <td>69926.png</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_id modality                                         input_text  \\\n",
       "0        trans     code  class GFG { static int countChars ( String str...   \n",
       "1  handwritten    image                                                NaN   \n",
       "2  handwritten    image                                                NaN   \n",
       "3        trans     code  import java . util . * ; public class VasyasCa...   \n",
       "4        trans     code  import java . io . * ; import java . util . * ...   \n",
       "5        trans     code  import java . util . * ;   public class Main {...   \n",
       "6  handwritten    image                                                NaN   \n",
       "7  handwritten    image                                                NaN   \n",
       "8  handwritten    image                                                NaN   \n",
       "9  handwritten    image                                                NaN   \n",
       "\n",
       "                                         output_text  stage input_image  \\\n",
       "0  def countChars ( string , n ) : NEW_LINE INDEN...  train         NaN   \n",
       "1                                                  a  train   38731.png   \n",
       "2                                              heads  train    6466.png   \n",
       "3  def main ( ) : d = int ( input ( ) ) n = int (...  train         NaN   \n",
       "4  n , m , k = map ( int , input ( ) . split ( ) ...  train         NaN   \n",
       "5  n , m = map ( int , input ( ) . split ( ) ) fr...  train         NaN   \n",
       "6                                              their  train    9665.png   \n",
       "7                                                the  train   29347.png   \n",
       "8                                             palely  train   27235.png   \n",
       "9                                                  с  valid   69926.png   \n",
       "\n",
       "  output_boxes  \n",
       "0          NaN  \n",
       "1          NaN  \n",
       "2          NaN  \n",
       "3          NaN  \n",
       "4          NaN  \n",
       "5          NaN  \n",
       "6          NaN  \n",
       "7          NaN  \n",
       "8          NaN  \n",
       "9          NaN  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготовка данных и сбор в единый DataFrame\n",
    "# #\n",
    "# Handwritten\n",
    "# #\n",
    "json_marking = json.load(open('handwritten/train_labels.json', 'rb'))\n",
    "marking = []\n",
    "for image_name, text in json_marking.items():\n",
    "    marking.append({\n",
    "        'path': image_name,\n",
    "        'text': text,\n",
    "        'lang': simple_detect_lang(text),\n",
    "    })\n",
    "df_handwritten = pd.DataFrame(marking)\n",
    "df_handwritten['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_handwritten.index, df_handwritten['lang']))\n",
    "df_handwritten.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Detection\n",
    "# #\n",
    "json_true_zsod = json.load(open('detection/true_zsOD.json', 'rb'))\n",
    "marking = []\n",
    "for image_name in json_true_zsod:\n",
    "    marking.extend([{\n",
    "        'task_id': 'detection',\n",
    "        'path': image_name,\n",
    "        'req': request,\n",
    "        'boxes': boxes,\n",
    "        'lang': 'en'\n",
    "    } for request, boxes in json_true_zsod[image_name].items() if boxes])\n",
    "df_detection = pd.DataFrame(marking)\n",
    "df_detection['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_detection.index, df_detection['lang']))\n",
    "df_detection.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Eval Detection #\n",
    "# #\n",
    "marking = []\n",
    "for image_name in json_true_zsod:\n",
    "    marking.append({\n",
    "        'task_id': 'detection',\n",
    "        'path': image_name,\n",
    "        'req': ';'.join([request for request in json_true_zsod[image_name].keys()]),\n",
    "        'boxes': [boxes for boxes in json_true_zsod[image_name].values()],\n",
    "        'lang': 'en'\n",
    "    })\n",
    "df_eval_detection = pd.DataFrame(marking)\n",
    "df_eval_detection['stage'] = 'test'\n",
    "# #\n",
    "# VQA\n",
    "# #\n",
    "json_questions = json.load(open('vqa/questions.json', 'rb'))\n",
    "json_true_vqa = json.load(open('vqa/true_VQA.json', 'rb'))\n",
    "marking = []\n",
    "for key in json_questions:\n",
    "    if json_true_vqa[key]['lang'] == 'en':\n",
    "        marking.append({\n",
    "            'path': str(json_questions[key]['image_id']) + '.jpg',\n",
    "            'question': json_questions[key]['question'],\n",
    "            'answer': json_true_vqa[key]['answer'],\n",
    "            'lang': json_true_vqa[key]['lang']\n",
    "        })\n",
    "df_vqa = pd.DataFrame(marking)\n",
    "df_vqa['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_vqa.index, df_vqa['lang']))\n",
    "df_vqa.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# C2C\n",
    "# #\n",
    "df_c2c = pd.read_json(path_or_buf='c2c/java-python.jsonl', lines=True)\n",
    "train, test = train_test_split(df_c2c, test_size=0.2)\n",
    "valid, test = train_test_split(test, test_size=0.05)\n",
    "\n",
    "df_c2c.loc[train.index.to_list(), 'stage'] = 'train'\n",
    "df_c2c.loc[valid.index.to_list(), 'stage'] = 'valid'\n",
    "df_c2c.loc[test.index.to_list(), 'stage'] = 'test'\n",
    "\n",
    "\n",
    "# #\n",
    "# Merge in common set\n",
    "# #\n",
    "dataset = []\n",
    "for image_name, text, stage in zip(df_handwritten['path'], df_handwritten['text'], df_handwritten['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'handwritten',   \n",
    "        'modality': 'image', \n",
    "        'input_image': image_name,\n",
    "        'output_text': text,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for java, python, stage in zip(df_c2c['java'], df_c2c['python'], df_c2c['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'trans',\n",
    "        'modality': 'code',    \n",
    "        'input_text': java,\n",
    "        'output_text': python,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for image_name, text_input, text_output, stage in zip(df_vqa['path'], df_vqa['question'], df_vqa['answer'], df_vqa['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'vqa', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_text': text_output,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for image_name, text_input, boxes, stage in zip(df_detection['path'], df_detection['req'], df_detection['boxes'], df_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for image_name, text_input, boxes, stage in zip(df_eval_detection['path'], df_eval_detection['req'], df_eval_detection['boxes'], df_eval_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "\n",
    "random.shuffle(dataset)\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e41b92f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['modality'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c121121",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 task_ids, \n",
    "                 input_images,\n",
    "                 input_texts,\n",
    "                 output_texts,\n",
    "                 output_boxes,\n",
    "                 ctc_labeling, \n",
    "                 tokenizer, \n",
    "                 stage, \n",
    "                 max_request_tokens_length,\n",
    "                 vqa_max_tokens_length, \n",
    "                 task_augs=None):\n",
    "        super().__init__()\n",
    "        self.task_ids = task_ids\n",
    "        \n",
    "        self.input_images = input_images\n",
    "        self.input_texts = input_texts\n",
    "        self.output_texts = output_texts\n",
    "        \n",
    "        self.task_augs = task_augs or {}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stage = stage\n",
    "\n",
    "        # handwritten[image]:\n",
    "        self.ctc_labeling = ctc_labeling\n",
    "        self.handwritten_image_w = 512\n",
    "        self.handwritten_image_h = 128\n",
    "\n",
    "        # code2code\n",
    "        self.code_max_length = 512\n",
    "        \n",
    "        # detection[image, text]:\n",
    "        self.max_request_tokens_length = max_request_tokens_length\n",
    "        self.output_boxes = output_boxes\n",
    "        \n",
    "        # vqa[image, text]:\n",
    "        self.vqa_max_tokens_length = vqa_max_tokens_length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_id = self.task_ids[idx]\n",
    "        if task_id == 'handwritten':\n",
    "            return self.get_handwritten_sample(idx)\n",
    "        elif task_id == 'trans':\n",
    "            return self.get_trans_sample(idx)\n",
    "        elif task_id == 'detection':\n",
    "            return self.get_detection_sample(idx)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.get_vqa_sample(idx)\n",
    "        return {'task_id': task_id}\n",
    "\n",
    "    def get_trans_sample(self, idx):\n",
    "            \n",
    "        source = self.input_texts[idx]\n",
    "        encoded_source = self.tokenizer.encode(str(source))\n",
    "        target = self.output_texts[idx]\n",
    "        encoded_target = self.tokenizer.encode(str(target))\n",
    "        \n",
    "        input_ids, input_labels = self.pad_and_get_mask(encoded_target, encoded_source, self.tokenizer)\n",
    "        input_ids, input_labels = torch.tensor(input_ids), torch.tensor(input_labels)\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'input_ids': input_ids,\n",
    "            'input_labels': input_labels,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "    def get_handwritten_sample(self, idx):\n",
    "        path = 'handwritten/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image, _ = self.resize_image(image)\n",
    "\n",
    "        gt_text = self.output_texts[idx]\n",
    "        encoded = self.ctc_labeling.encode(gt_text)\n",
    "\n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('handwritten')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        ##########\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image': image,\n",
    "            'gt_text': gt_text,\n",
    "            'encoded': torch.tensor(encoded, dtype=torch.int32),\n",
    "        }\n",
    "    \n",
    "    def get_detection_sample(self, idx):\n",
    "        path = 'detection/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_h, image_w, _ = image.shape\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('detection')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        ##########\n",
    "        image_name = self.input_images[idx]\n",
    "        \n",
    "        ## Input tokens ##\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_text = self.input_texts[idx]\n",
    "            input_tokens = self.tokenizer.encode_plus(input_text)\n",
    "            pad_len = self.max_request_tokens_length - len(input_tokens['input_ids'])\n",
    "            input_tokens['input_ids'] += [self.tokenizer.pad_token_id] * pad_len\n",
    "            input_tokens['attention_mask'] += [0] * pad_len\n",
    "            input_ids = torch.tensor(input_tokens['input_ids'])\n",
    "            attention_mask = torch.tensor(input_tokens['attention_mask'])\n",
    "        else:\n",
    "            input_texts = self.input_texts[idx].split(';')\n",
    "            input_ids = list(map(self.tokenizer.encode, input_texts))\n",
    "            attention_mask = [[1 for _ in input_token] for input_token in input_ids]\n",
    "            input_ids = [torch.tensor(input_id) for input_id in input_ids]\n",
    "            attention_mask = [torch.tensor(mask) for mask in attention_mask]\n",
    "        ###########\n",
    "        \n",
    "        ## Boxes ##\n",
    "        output_boxes = self.output_boxes[idx]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            output_boxes = torch.tensor(output_boxes, dtype=torch.float32)\n",
    "            output_boxes[:, 0] /= image_w\n",
    "            output_boxes[:, 1] /= image_h\n",
    "            output_boxes[:, 2] /= image_w\n",
    "            output_boxes[:, 3] /= image_h\n",
    "        else:\n",
    "            output_boxes = {\n",
    "                input_text: boxes for input_text, boxes in zip(input_texts, output_boxes)\n",
    "            }\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'boxes': output_boxes,\n",
    "            'size': (image_h, image_w)\n",
    "        }\n",
    "    \n",
    "    def get_vqa_sample(self, idx): \n",
    "        path = 'vqa/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('vqa')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        ##########\n",
    "        image_name = self.input_images[idx]\n",
    "        \n",
    "        ## Question and Answer ##\n",
    "        input_text = self.input_texts[idx]\n",
    "        input_tokens = self.tokenizer.encode(input_text)\n",
    "        output_text = self.output_texts[idx]\n",
    "        output_tokens = self.tokenizer.encode(output_text)\n",
    "        \n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id] + output_tokens + [self.tokenizer.eos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2] * (len(output_tokens) + 1) + [0]\n",
    "            \n",
    "            pad_len = self.vqa_max_tokens_length - len(input_ids)\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
    "            labels += [0] * pad_len\n",
    "        else:\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2]\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'labels': torch.tensor(labels),\n",
    "            'target': output_text\n",
    "        }\n",
    "\n",
    "\n",
    "    def resize_image(self, image):\n",
    "        image, coef = resize_if_need(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        image = make_img_padding(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        return image, coef\n",
    "    \n",
    "    def pad_and_get_mask(self, target, source, tokenizer):\n",
    "        if self.stage == 'test':\n",
    "            target = []\n",
    "        while len(target) + len(source) + 2 > self.code_max_length:\n",
    "            if len(target) > len(source):\n",
    "                target = target[:-1]\n",
    "            else:\n",
    "                source = source[:-1]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            inputs = source + [tokenizer.bos_token_id] + target + [tokenizer.eos_token_id]\n",
    "            labels = [1] * len(source) + [2] * (len(target) + 1) + [0]\n",
    "\n",
    "        else:\n",
    "            inputs = source + [tokenizer.bos_token_id]\n",
    "            labels = [1] * len(source) + [2]\n",
    "            \n",
    "            return inputs, labels\n",
    "        \n",
    "        assert len(inputs) <= self.code_max_length\n",
    "        pad_len = self.code_max_length - len(inputs)\n",
    "        inputs += [tokenizer.pad_token_id] * pad_len\n",
    "        labels += [0] * pad_len\n",
    "        assert len(inputs) == len(labels)\n",
    "        \n",
    "        return inputs, labels\n",
    "\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return self.task_ids.shape[0]\n",
    "\n",
    "    def get_task_labels(self):\n",
    "        return list(self.task_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebb6902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:967: FutureWarning: This class has been deprecated. Please use ImageCompression\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "task_augs = {\n",
    "    'handwritten': A.Compose([\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "        A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "        A.JpegCompression(quality_lower=75, p=0.5),\n",
    "    ], p=1.0),\n",
    "    'vqa': A.Compose([\n",
    "        A.Resize(800, 800, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0),\n",
    "    'detection': A.Compose([\n",
    "        A.Resize(800, 800, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3026efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['stage'] == 'train']\n",
    "df_valid = df[df['stage'] == 'valid']\n",
    "df_eval = df[df['stage'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12f6586d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Some weights of the model checkpoint at microsoft/CodeGPT-small-py were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50002, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготовка предобученной модели и токенизатора, а также CTC Labeling для задачи распознавания рукописного текста\n",
    "CHARS = ' !\"#&\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ' + \\\n",
    "    '[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'\n",
    "ctc_labeling = CTCLabeling(CHARS)\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained('gpt2')\n",
    "gpt_model.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "643b46d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetRetriever(\n",
    "    task_ids=df_train['task_id'].values,\n",
    "    input_images=df_train['input_image'].values,\n",
    "    input_texts=df_train['input_text'].values,\n",
    "    output_texts=df_train['output_text'].values,\n",
    "    output_boxes=df_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "valid_dataset = DatasetRetriever(\n",
    "    task_ids=df_valid['task_id'].values,\n",
    "    input_images=df_valid['input_image'].values,\n",
    "    input_texts=df_valid['input_text'].values,\n",
    "    output_texts=df_valid['output_text'].values,\n",
    "    output_boxes=df_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "eval_dataset = DatasetRetriever(\n",
    "    task_ids=df_eval['task_id'].values,\n",
    "    input_images=df_eval['input_image'].values,\n",
    "    input_texts=df_eval['input_text'].values,\n",
    "    output_texts=df_eval['output_text'].values,\n",
    "    output_boxes=df_eval['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8443c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как выглядят семплы для каждой задачи\n",
    "def demo_sample(sample):\n",
    "    if sample['task_id'] == 'handwritten':\n",
    "        print('[gt_text]:',sample['gt_text'])\n",
    "        return io.imshow(sample['image'].permute(1,2,0).numpy())\n",
    "    elif sample['task_id'] == 'trans':\n",
    "        print('[source_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[target_text]:', sample['target'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'detection':\n",
    "        print('[input_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[boxes]:', sample['boxes'].numpy())\n",
    "        return\n",
    "    elif sample['task_id'] == 'vqa':\n",
    "        print('[question and answer]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        return\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "199bbb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_text]: cloudy gray sky\n",
      "[boxes]: [[ 0.00125    -0.00333333  0.995       0.975     ]]\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'detection').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b34dca2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[source_text]: class GFG { static int countChars ( String str, int n ) { int i = 0, cnt = 0 ; while ( i < n ) { if ( str. charAt ( i ) == '0' ) i += 1 ; else i += 2 ; cnt += 1 ; } return cnt ; } public static void main ( String [ ] args ) { String str = \"11010\" ; int n = str. length ( ) ; System. out. println ( countChars ( str, n ) ) ; } }\n",
      "def countChars ( string, n ) : NEW_LINE INDENT i = 0 ; cnt = 0 ; NEW_LINE while ( i < n ) : NEW_LINE INDENT if ( string [ i ] == '0' ) : NEW_LINE INDENT i += 1 ; NEW_LINE DEDENT else : NEW_LINE INDENT i += 2 ; NEW_LINE DEDENT cnt += 1 ; NEW_LINE DEDENT return cnt ; NEW_LINE DEDENT if __name__ == \" _ _ main _ _ \" : NEW_LINE INDENT string = \"11010\" ; NEW_LINE n = len ( string ) ; NEW_LINE print ( countChars ( string, n ) ) ; NEW_LINE DEDENT\n",
      "\n",
      "[target_text]: def countChars ( string , n ) : NEW_LINE INDENT i = 0 ; cnt = 0 ; NEW_LINE while ( i < n ) : NEW_LINE INDENT if ( string [ i ] == '0' ) : NEW_LINE INDENT i += 1 ; NEW_LINE DEDENT else : NEW_LINE INDENT i += 2 ; NEW_LINE DEDENT cnt += 1 ; NEW_LINE DEDENT return cnt ; NEW_LINE DEDENT if __name__ == \" _ _ main _ _ \" : NEW_LINE INDENT string = \"11010\" ; NEW_LINE n = len ( string ) ; NEW_LINE print ( countChars ( string , n ) ) ; NEW_LINE DEDENT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'trans').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7ccc36dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[question and answer]: How many hats are there?1\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'vqa').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "38d26177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gt_text]: discrimination\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f92c81aca10>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'handwritten').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ace059ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_collate_fn(batch):\n",
    "    \"\"\" fusion brain collate fn \"\"\"\n",
    "    encoded, encoded_length, htr_images, gt_texts = [], [], [], [] # handwritten[image]\n",
    "    code_input_ids, code_input_labels, code_targets = [], [], [] #code\n",
    "    images, input_ids, attention_masks, boxes, labels, targets, size = [], [], [], [], [], [], []  # detection_vqa[image, text]\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        if sample['task_id'] == 'handwritten':\n",
    "            encoded.append(sample['encoded'])\n",
    "            encoded_length.append(sample['encoded'].shape[0])\n",
    "            htr_images.append(sample['image'])\n",
    "            gt_texts.append(sample['gt_text'])\n",
    "        elif sample['task_id'] == 'trans':\n",
    "            code_input_ids.append(sample['input_ids'])\n",
    "            code_input_labels.append(sample['input_labels'])\n",
    "            code_targets.append(sample['target'])\n",
    "        elif sample['task_id'] == 'detection':\n",
    "            images.append(sample['image'])\n",
    "            input_ids.append(sample['input_ids'])\n",
    "            attention_masks.append(sample['attention_mask'])\n",
    "            boxes.append(sample['boxes'])\n",
    "            size.append(sample['size'])\n",
    "        elif sample['task_id'] == 'vqa':\n",
    "            images.append(sample['image'])\n",
    "            input_ids.append(sample['input_ids'])\n",
    "            labels.append(sample['labels'])\n",
    "            targets.append(sample['target'])\n",
    "        \n",
    "    if htr_images:\n",
    "        htr_images = pad_sequence(htr_images, batch_first=True)\n",
    "        encoded, encoded_length = pad_sequence(encoded, batch_first=True), torch.tensor(encoded_length)\n",
    "    if images:\n",
    "        images = torch.stack(images)        \n",
    "    if attention_masks and torch.is_tensor(attention_masks[0]):\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "        attention_masks = torch.stack(attention_masks)\n",
    "    elif attention_masks:\n",
    "        input_ids = [input_id.unsqueeze(0) for input_id in input_ids[0]]\n",
    "        attention_masks = [attention_mask.unsqueeze(0) for attention_mask in attention_masks[0]]\n",
    "    if labels:\n",
    "        input_ids = pad_sequence(input_ids, batch_first=True)\n",
    "        labels = pad_sequence(labels, batch_first=True)    \n",
    "    if code_input_ids:\n",
    "        code_input_ids = pad_sequence(code_input_ids, batch_first=True)\n",
    "        code_input_labels = pad_sequence(code_input_labels, batch_first=True)\n",
    "    return (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, code_targets), (images, input_ids, attention_masks, boxes, labels, targets, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11051ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2FusionBrain(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 gpt_model, \n",
    "                 attention_config, \n",
    "                 handwritten_config, \n",
    "                 vqa_config, \n",
    "                 detection_config, \n",
    "                 **freeze_gpt_kwargs):\n",
    "        super().__init__()\n",
    "        self.gpt_model = gpt_model\n",
    "        self.embedding_size = self.gpt_model.config.n_embd\n",
    "        self.freeze_gpt(**freeze_gpt_kwargs)\n",
    "\n",
    "        # handwritten[image] input/output layers:\n",
    "        self.handwritten_config = handwritten_config\n",
    "        self.handwritten_input_layer = self._build_input_net(\n",
    "            input_dim=handwritten_config['patch_w']*handwritten_config['patch_h']*3,\n",
    "            in_layer_sizes=handwritten_config['in_layer_sizes'],\n",
    "            orth_gain=handwritten_config['orth_gain'],\n",
    "            dropout=handwritten_config['dropout'],\n",
    "        )\n",
    "        self.handwritten_lstm = nn.LSTM(\n",
    "            self.embedding_size, self.embedding_size // 2,\n",
    "            handwritten_config['lstm_num_layers'], dropout=handwritten_config['dropout'],\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.handwritten_output_layer = self._build_output_net(\n",
    "            output_dim=handwritten_config['output_dim'],\n",
    "            out_layer_sizes=handwritten_config['out_layer_sizes'],\n",
    "            dropout=handwritten_config['dropout'],\n",
    "        )\n",
    "        print('=== HANDWRITTEN TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.handwritten_input_layer,\n",
    "            self.gpt_model, \n",
    "            self.handwritten_lstm, \n",
    "            self.handwritten_output_layer,\n",
    "        ])\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "\n",
    "        # code2code\n",
    "        self.beam_size=3\n",
    "        self.sos_id=self.gpt_model.config.bos_token_id\n",
    "        self.eos_id=self.gpt_model.config.eos_token_id\n",
    "        self.lm_head = nn.Linear(self.gpt_model.config.n_embd, self.gpt_model.config.vocab_size, bias=False)\n",
    "\n",
    "        print('=== C2C TASK ===')\n",
    "        self._calculate_trainable_params([self.gpt_model, self.lm_head])\n",
    "        print('=== === === === ===')\n",
    "        \n",
    "        ## zhOD[image, text] and VQA[image, text] layers:\n",
    "        self.attention_config = attention_config\n",
    "        self.backbone = Resnet50Backbone(pretrained=True)\n",
    "        self.input_proj = nn.Conv2d(self.backbone.num_channels, self.embedding_size, kernel_size=1)\n",
    "        self.cross_attention = nn.ModuleList([\n",
    "            CrossAttentionLayer(self.embedding_size, attention_config['num_heads'], attention_config['pf_dim'])\n",
    "            for _ in range(attention_config['num_attention_layers'])\n",
    "        ])\n",
    "        #####\n",
    "        \n",
    "        # detection[image, text] input/output layers:\n",
    "        self.detection_config = detection_config\n",
    "        self.detection_max_pool = nn.AdaptiveMaxPool1d(detection_config['num_queries'])\n",
    "        self.bbox_embed = MLP(self.embedding_size, self.embedding_size, 5, detection_config['num_mlp_layers'])\n",
    "        print('=== DETECTION TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.backbone,\n",
    "            self.gpt_model, \n",
    "            self.input_proj,\n",
    "            self.detection_max_pool,\n",
    "            self.cross_attention,\n",
    "            self.bbox_embed\n",
    "        ])\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "         # vqa[image, text] input/output layers:\n",
    "        self.vqa_config = vqa_config\n",
    "        self.vqa_max_pool = nn.AdaptiveMaxPool1d(vqa_config['num_queries'])\n",
    "        self.tokens_embed = nn.Linear(self.embedding_size, vqa_config['tokens_num'])\n",
    "        print('=== VQA TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.backbone,\n",
    "            self.gpt_model, \n",
    "            self.input_proj,\n",
    "            self.vqa_max_pool,\n",
    "            self.cross_attention,\n",
    "            self.bbox_embed\n",
    "        ])\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "        print('=== COMMON PARAMS ===')\n",
    "        self._calculate_common_params()\n",
    "        print('=== === === === ===')\n",
    "\n",
    "\n",
    "    def forward(self, task_id, **kwargs):\n",
    "        if task_id == 'handwritten':\n",
    "            return self.forward_handwritten(**kwargs)\n",
    "        elif task_id == 'trans':\n",
    "            return self.forward_trans(**kwargs)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.forward_vqa(**kwargs)\n",
    "        elif task_id == 'detection':\n",
    "            return self.forward_detection(**kwargs)\n",
    "\n",
    "    def forward_trans(self, input_ids, input_labels=None, eval_bleu=False, past=None):\n",
    "        if not eval_bleu:\n",
    "            attn_mask = torch.tensor(input_labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "            attn_mask = attn_mask.to(input_labels.device)\n",
    "            outputs = self.gpt_model(input_ids, attention_mask=attn_mask)\n",
    "            x = self.lm_head(outputs[0])\n",
    "            return x\n",
    "        else:\n",
    "            if past != None:\n",
    "                outputs = self.gpt_model(input_ids, past_key_values=past)\n",
    "                logits = self.lm_head(outputs[0])\n",
    "                return logits, outputs[1]\n",
    "            else:\n",
    "                outputs = self.gpt_model(input_ids)[1]\n",
    "                return outputs\n",
    "\n",
    "    def forward_handwritten(self, images):\n",
    "        x = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.handwritten_config['patch_h'], p2=self.handwritten_config['patch_w'])\n",
    "        x = x.squeeze(2)\n",
    "        x = self.handwritten_input_layer(x)\n",
    "        # Fusion Brain\n",
    "        transformer_outputs = self.gpt_model(inputs_embeds=x, output_hidden_states=True)\n",
    "        x = transformer_outputs.last_hidden_state\n",
    "        #####\n",
    "        x, _ = self.handwritten_lstm(x)        \n",
    "        x = self.handwritten_output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward_vqa(self, images, tokens, labels):\n",
    "        back_out = self.backbone(images)\n",
    "        patchs = self.input_proj(back_out).flatten(-2)\n",
    "        patchs = self.vqa_max_pool(patchs).transpose(-1, -2)\n",
    "        attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "        attention_mask = attention_mask.to(labels.device)\n",
    "        # Fusion Brain\n",
    "        img_emb = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_emb = self.gpt_model(input_ids=tokens, attention_mask=attention_mask).last_hidden_state\n",
    "        #####\n",
    "        for layer in self.cross_attention:\n",
    "            tokens_emb, _ = layer(tokens_emb, img_emb)\n",
    "            \n",
    "        output_logits = self.tokens_embed(tokens_emb)\n",
    "        \n",
    "        return output_logits\n",
    "    \n",
    "    def forward_detection(self, images, tokens, attention_mask):\n",
    "        back_out = self.backbone(images)\n",
    "        patchs = self.input_proj(back_out).flatten(-2)\n",
    "        patchs = self.detection_max_pool(patchs).transpose(-1, -2)\n",
    "        # Fusion Brain\n",
    "        img_emb = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_emb = self.gpt_model(input_ids=tokens, attention_mask=attention_mask).last_hidden_state\n",
    "        #####\n",
    "        text_mask = attention_mask.type(torch.bool)\n",
    "        for layer in self.cross_attention:\n",
    "            img_emb, _ = layer(img_emb, tokens_emb, ~text_mask)\n",
    "            \n",
    "        output_logits = self.bbox_embed(img_emb).sigmoid()\n",
    "        normalized_tokens_emb = F.normalize(tokens_emb, p=2, dim=-1)\n",
    "        normalized_tokens_emb = normalized_tokens_emb * attention_mask.unsqueeze(-1)\n",
    "        normalized_img_emb =  F.normalize(img_emb, p=2, dim=-1)\n",
    "        out = {\n",
    "            'pred_logits': output_logits,\n",
    "            'proj_tokens': normalized_tokens_emb,\n",
    "            'proj_queries': normalized_img_emb,\n",
    "        }\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def freeze_gpt(self, freeze_pos=True, freeze_ln=True, freeze_attn=True, freeze_ff=True, freeze_other=True):\n",
    "        for name, p in self.gpt_model.named_parameters():\n",
    "            name = name.lower()\n",
    "            if 'ln' in name or 'norm' in name:\n",
    "                p.requires_grad = not freeze_ln\n",
    "            elif 'wpe' in name or 'position_embeddings' in name or 'pos_drop' in name:\n",
    "                p.requires_grad = not freeze_pos\n",
    "            elif 'mlp' in name:\n",
    "                p.requires_grad = not freeze_ff\n",
    "            elif 'attn' in name:\n",
    "                p.requires_grad = not freeze_attn\n",
    "            else:\n",
    "                p.requires_grad = not freeze_other\n",
    "\n",
    "    def _build_input_net(self, input_dim, in_layer_sizes=None, orth_gain=1.41, dropout=0.1):\n",
    "        \"\"\" вспомогательный метод для сборки input слоя, который приводит размер входящих данный к эмбеддингу gpt \"\"\"\n",
    "        in_layer_sizes = [] if not in_layer_sizes else in_layer_sizes\n",
    "        in_layers = []\n",
    "        last_output_size = input_dim\n",
    "        for size in in_layer_sizes:\n",
    "            layer = nn.Linear(last_output_size, size)\n",
    "            if orth_gain is not None:\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=orth_gain)\n",
    "            layer.bias.data.zero_()\n",
    "\n",
    "            in_layers.append(layer)\n",
    "            in_layers.append(nn.ReLU())\n",
    "            in_layers.append(nn.Dropout(dropout))\n",
    "            last_output_size = size\n",
    "\n",
    "        final_linear = nn.Linear(last_output_size, self.embedding_size)\n",
    "        if orth_gain is not None:\n",
    "            torch.nn.init.orthogonal_(final_linear.weight, gain=orth_gain)\n",
    "        final_linear.bias.data.zero_()\n",
    "\n",
    "        in_layers.append(final_linear)\n",
    "        in_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        return nn.Sequential(*in_layers)\n",
    "    \n",
    "    def _build_output_net(self, output_dim, embedding_size=None, out_layer_sizes=None, dropout=0.1):\n",
    "        \"\"\" вспомогательный метод для сборки output слоя \"\"\"\n",
    "        out_layer_sizes = [] if not out_layer_sizes else out_layer_sizes\n",
    "        out_layers = []\n",
    "        last_output_size = embedding_size or self.embedding_size\n",
    "        for size in out_layer_sizes:\n",
    "            out_layers.append(nn.Linear(last_output_size, size))\n",
    "            out_layers.append(nn.ReLU())\n",
    "            out_layers.append(nn.Dropout(dropout))\n",
    "            last_output_size = size\n",
    "        out_layers.append(nn.Linear(last_output_size, output_dim))\n",
    "        return nn.Sequential(*out_layers)\n",
    "\n",
    "    def _calculate_trainable_params(self, layers):\n",
    "        trainable_params, all_used_params = 0, 0\n",
    "        for layer in layers:\n",
    "            trainable_params += sum(p.numel() for p in layer.parameters() if p.requires_grad)\n",
    "            all_used_params += sum(p.numel() for p in layer.parameters())        \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print(' all_used_params:', all_used_params)\n",
    "        print('               %:', round(trainable_params/all_used_params*100, 2))\n",
    "\n",
    "    def _calculate_common_params(self):\n",
    "        all_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        common_params = sum(p.numel() for p in self.gpt_model.parameters())\n",
    "        print('common_params:', common_params)\n",
    "        print('   all_params:', all_params)\n",
    "        print('            %:', round(common_params/all_params*100, 2))     \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print('               %:', round(trainable_params/all_params*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e228fc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_config = {\n",
    "    'patch_w': 8,\n",
    "    'patch_h': 128,\n",
    "    'in_layer_sizes': [8*128*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'lstm_num_layers': 3,\n",
    "    'output_dim': len(ctc_labeling), # 152\n",
    "}\n",
    "\n",
    "attention_config = {\n",
    "    'num_attention_layers': 3,\n",
    "    'num_heads': 8,\n",
    "    'pf_dim': 2048,\n",
    "}\n",
    "\n",
    "vqa_config = {\n",
    "    'num_queries': 100,\n",
    "    'tokens_num': len(gpt_tokenizer),\n",
    "}\n",
    "\n",
    "detection_config = {\n",
    "    'num_queries': 8,\n",
    "    'num_mlp_layers': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40c35f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HANDWRITTEN TASK ===\n",
      "trainable_params: 22494680\n",
      " all_used_params: 146738648\n",
      "               %: 15.33\n",
      "=== === === === ===\n",
      "=== C2C TASK ===\n",
      "trainable_params: 38401536\n",
      " all_used_params: 162645504\n",
      "               %: 23.61\n",
      "=== === === === ===\n",
      "=== DETECTION TASK ===\n",
      "trainable_params: 29810477\n",
      " all_used_params: 154054445\n",
      "               %: 19.35\n",
      "=== === === === ===\n",
      "=== VQA TASK ===\n",
      "trainable_params: 29810477\n",
      " all_used_params: 154054445\n",
      "               %: 19.35\n",
      "=== === === === ===\n",
      "=== COMMON PARAMS ===\n",
      "common_params: 124243968\n",
      "   all_params: 253402199\n",
      "            %: 49.03\n",
      "trainable_params: 129158231\n",
      "               %: 50.97\n",
      "=== === === === ===\n"
     ]
    }
   ],
   "source": [
    "model = GPT2FusionBrain(\n",
    "    gpt_model,\n",
    "    attention_config=attention_config,\n",
    "    handwritten_config=handwritten_config,\n",
    "    vqa_config=vqa_config,\n",
    "    detection_config=detection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e0445ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Скрипты для обучения и оценки качества модели\n",
    "\n",
    "\n",
    "class FusionBrainExperiment(TorchGPUExperiment):\n",
    "\n",
    "    handwritten_criterion = torch.nn.CTCLoss(zero_infinity=True)\n",
    "    ctc_labeling = ctc_labeling\n",
    "    detection_criterion = DetectionCriterion(['boxes', 'classification'], 0.07)\n",
    "    detection_loss_weights = [1.0, 1.0, 1.0]\n",
    "    vqa_criterion = nn.CrossEntropyLoss()\n",
    "    c2c_criterion = torch.nn.CrossEntropyLoss()\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "    \n",
    "#     def custom_action_before_train_one_epoch(self, test_loader):\n",
    "#         run_evaluation(test_loader, self.model, tokenizer=self.tokenizer)\n",
    "        \n",
    "#     def _custom_action_before_train_one_epoch(self):\n",
    "#         self._wipe_memory()\n",
    "#         self.custom_action_before_train_one_epoch(test_loader)\n",
    "\n",
    "    def calculate_handwritten_metrics(self, gt_texts, outputs):\n",
    "        pred_texts = []\n",
    "        for encoded in outputs.argmax(2).data.cpu().numpy():\n",
    "            pred_texts.append(self.ctc_labeling.decode(encoded))\n",
    "        texts = [self.ctc_labeling.preprocess(text) for text in gt_texts]\n",
    "        return {\n",
    "            'h_cer': cer(pred_texts, texts),\n",
    "            'h_wer': wer(pred_texts, texts),\n",
    "            'h_acc': string_accuracy(pred_texts, texts),\n",
    "        }\n",
    "\n",
    "    def handle_one_batch(self, batch):\n",
    "        (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, _), (images, input_ids, attention_masks, boxes, labels, targets, size) = batch\n",
    "        sum_bs = len(htr_images) + len(code_input_ids) + len(images)\n",
    "        losses = []\n",
    "        metrics = {}\n",
    "\n",
    "        if len(htr_images) > 0:\n",
    "            bs = htr_images.shape[0]\n",
    "            images = htr_images.to(self.device, dtype=torch.float32)\n",
    "            encoded_length = encoded_length.to(self.device, dtype=torch.int32)\n",
    "            encoded = encoded.to(self.device, dtype=torch.int32)\n",
    "            handwritten_outputs = self.model('handwritten', images=images)\n",
    "            preds_size = torch.IntTensor([handwritten_outputs.size(1)] * bs)\n",
    "            preds = handwritten_outputs.log_softmax(2).permute(1, 0, 2)\n",
    "            handwritten_loss = self.handwritten_criterion(preds, encoded, preds_size, encoded_length)\n",
    "            handwritten_metrics = self.calculate_handwritten_metrics(gt_texts, handwritten_outputs)\n",
    "            metrics.update(handwritten_metrics)\n",
    "            metrics['h_loss'] = handwritten_loss.detach().cpu().item()\n",
    "            losses.append(handwritten_loss)\n",
    "\n",
    "        if len(code_input_ids) > 0:\n",
    "            bs = code_input_ids.shape[0]\n",
    "            code_input_ids = code_input_ids.to(self.device, dtype=torch.long) \n",
    "            code_input_labels = code_input_labels.to(self.device, dtype=torch.long) \n",
    "            loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('trans', input_ids=code_input_ids, input_labels=code_input_labels)\n",
    "            c_labels = code_input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = c_labels[..., 1:].contiguous()\n",
    "            \n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            c2c_loss = self.c2c_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['c2c_loss'] = c2c_loss.detach().cpu().item()\n",
    "            losses.append(c2c_loss)\n",
    "            \n",
    "        if len(labels) > 0:\n",
    "            images = images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = input_ids.to(self.device, dtype=torch.long)\n",
    "            labels = labels.to(self.device, dtype=torch.float32)\n",
    "            loss_mask = torch.tensor(labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('vqa', images=images, tokens=input_ids, labels=labels)\n",
    "            labels = input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            vqa_loss = self.vqa_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['vqa_loss'] = vqa_loss.detach().cpu().item()\n",
    "            losses.append(vqa_loss)\n",
    "            \n",
    "        if len(boxes) > 0:\n",
    "            images = images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = input_ids.to(self.device, dtype=torch.long) \n",
    "            attention_masks = attention_masks.to(self.device, dtype=torch.long) \n",
    "            boxes = [boxes_per_label.to(self.device, dtype=torch.float) for boxes_per_label in boxes]\n",
    "            detection_outputs = self.model('detection', images=images, tokens=input_ids, attention_mask=attention_masks)\n",
    "            detection_loss = self.detection_criterion(detection_outputs, boxes)\n",
    "            #detection_loss = sum([\n",
    "            #    loss * weight for loss, weight in zip(detection_loss.values(), self.detection_loss_weights)\n",
    "            #])\n",
    "            #metrics['detection_acc'] = acc(targets.argmax(axis=1), sentiment_outputs)\n",
    "            metrics['loss_giou'] = detection_loss['loss_giou'].detach().cpu().item()\n",
    "            metrics['loss_bbox'] = detection_loss['loss_bbox'].detach().cpu().item()\n",
    "            #metrics['loss_contrastive'] = detection_loss['loss_contrastive'].detach().cpu().item()\n",
    "            metrics['loss_classification'] = detection_loss['loss_classification'].detach().cpu().item()\n",
    "            losses.append(detection_loss['loss_giou'])\n",
    "            losses.append(detection_loss['loss_bbox'])\n",
    "            #losses.append(detection_loss['loss_contrastive'])\n",
    "            losses.append(detection_loss['loss_classification'])\n",
    "\n",
    "        #loss = sum(losses)\n",
    "        loss = sum([loss * weight for loss, weight in zip(losses, self.detection_loss_weights)])\n",
    "        \n",
    "        #print(loss)\n",
    "        self.metrics.update(loss=loss.detach().cpu().item(), **metrics)\n",
    "        \n",
    "        #self.metrics.update(loss=loss.item(), **metrics)\n",
    "\n",
    "        if self.is_train:\n",
    "            loss.backward()\n",
    "            self.optimizer_step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "def run_evaluation(loader, model, tokenizer=None, device=torch.device('cuda:0')):\n",
    "    result = []\n",
    "    true_json_detection = {}\n",
    "    pred_json_detection = {}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, code_targets), (images, input_ids, attention_masks, boxes, labels, targets, size) = batch\n",
    "            if len(htr_images) > 0:\n",
    "                images = htr_images.to(device, dtype=torch.float32)\n",
    "                handwritten_outputs = model('handwritten', images=images)\n",
    "                for encoded, gt_text in zip(handwritten_outputs.argmax(2).data.cpu().numpy(), gt_texts):\n",
    "                    pred_text = ctc_labeling.decode(encoded)\n",
    "                    result.append({\n",
    "                        'task_id': 'handwritten',\n",
    "                        'gt_output': gt_text,\n",
    "                        'pred_output': pred_text,\n",
    "                    })\n",
    "\n",
    "            if len(code_input_ids) > 0:\n",
    "                code_input_ids = code_input_ids.to(device, dtype=torch.long)\n",
    "                code_input_labels = code_input_labels.to(device, dtype=torch.long)\n",
    "                loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "                loss_mask = loss_mask.to(device)\n",
    "                hidden_states = model('trans', input_ids=code_input_ids, input_labels=code_input_labels, eval_bleu=True)\n",
    "                bleu_score, _ = eval_bleu(model, hidden_states, input_ids=code_input_ids, beam_size=5, tokenizer=tokenizer, targets=code_targets)\n",
    "                result.append({\n",
    "                        'task_id': 'trans',\n",
    "                        'true_text': code_targets,\n",
    "                        'bleu_score': bleu_score,\n",
    "                })\n",
    "                \n",
    "            if len(labels) > 0:\n",
    "                images = images.to(device, dtype=torch.float32)\n",
    "                input_ids = input_ids.to(device, dtype=torch.long)\n",
    "                labels = labels.to(device, dtype=torch.float) \n",
    "                attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "                attention_mask = attention_mask.to(labels.device)\n",
    "                vqa_outputs = vqa_evaluation(model, images, input_ids, attention_mask, 10)\n",
    "                for target, pred_labels in zip(targets, vqa_outputs.argmax(-1).cpu().numpy()):\n",
    "                    result.append({\n",
    "                        'task_id': 'vqa',\n",
    "                        'gt_output': target,\n",
    "                        'pred_output': gpt_tokenizer.decode(pred_labels).split(gpt_tokenizer.eos_token)[0],\n",
    "                    })\n",
    "                    \n",
    "            if len(boxes) > 0:\n",
    "                images = images.to(device, dtype=torch.float32)\n",
    "                input_ids = [input_id.to(device, dtype=torch.long) for input_id in input_ids]\n",
    "                attention_masks = [attention_mask.to(device, dtype=torch.long) for attention_mask in attention_masks]\n",
    "                detection_outputs = detection_evaluation(model, images, input_ids, attention_masks, 0.1)\n",
    "                for image_name, boxes_for_img in zip(image_names, boxes):\n",
    "                    true_json_detection[image_names] = boxes_for_img\n",
    "                    pred_json_detection[image_names] = {\n",
    "                        input_text: output.cpu().tolist()\n",
    "                        for input_text, output in zip(boxes_for_img.keys(), detection_outputs)\n",
    "                    }\n",
    "                    image_iter += 1\n",
    "                \n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    handwritten_result = result[result['task_id'] == 'handwritten']\n",
    "    if handwritten_result.shape[0]:\n",
    "        print('= Handwritten =')\n",
    "        print('CER:', round(cer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('WER:', round(wer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('ACC:', round(string_accuracy(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    trans_result = result[result['task_id'] == 'trans']   \n",
    "    if trans_result.shape[0]:\n",
    "        print('== C2C ==')\n",
    "        print('meanBLEU:', np.mean(trans_result['bleu_score']))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    vqa_result = result[result['task_id'] == 'vqa']\n",
    "    if vqa_result.shape[0]:\n",
    "        print('== VQA ==')\n",
    "        print('ACC:', round(vqa_evaluate(vqa_result), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    \n",
    "    if len(true_json_detection):\n",
    "        print('== Detection ==')\n",
    "        print('ACC:', round(detection_evaluate(true_json_detection, pred_json_detection), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_model(model, experiment_name):\n",
    "    paths = sorted(glob(f'./saved_models/{experiment_name}*'))\n",
    "    if len(paths) == 0:\n",
    "        print('Warning! Model not found')\n",
    "        return model\n",
    "    checkpoint_path = paths[-1] + '/last.pt'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     metrics = defaultdict(list)\n",
    "#     for epoch in range(len(checkpoint['metrics_state_dict']['train_metrics'])):\n",
    "#         train_metrics = checkpoint['metrics_state_dict']['train_metrics'][epoch]['avg']\n",
    "#         valid_metrics = checkpoint['metrics_state_dict']['valid_metrics'][epoch]['avg']\n",
    "#         for key in train_metrics.keys():\n",
    "#             metrics[f'train_{key}'].append(train_metrics[key])\n",
    "#             metrics[f'valid_{key}'].append(valid_metrics[key])\n",
    "#         metrics['epoch'].append(epoch)        \n",
    "#     metrics = pd.DataFrame(metrics)\n",
    "    return model, metrics\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f605b867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/405 [00:00<?, ?it/s]/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:145: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 405/405 [14:29<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== C2C ==\n",
      "meanBLEU: 0.7914567901234567\n",
      "=== === === ===\n",
      "\n",
      "2021-10-16T15:56:33.301058\n",
      "lr: 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:57: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2152/2041393953.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mtest_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     )\n\u001b[0;32m---> 27\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 self._print(\n",
      "\u001b[0;32m/tmp/ipykernel_2152/736786849.py\u001b[0m in \u001b[0;36mhandle_one_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения c2c[code]\n",
    "\n",
    "DEMO_LOGS = False \n",
    "\n",
    "trans_train = df[(df['task_id'] == 'trans') & (df['stage'] == 'train')]\n",
    "trans_valid = df[(df['task_id'] == 'trans') & (df['stage'] == 'valid')]\n",
    "trans_test = df[(df['task_id'] == 'trans') & (df['stage'] == 'test')]\n",
    "\n",
    "\n",
    "trans_train_dataset = DatasetRetriever(\n",
    "    task_ids=trans_train['task_id'].values,\n",
    "    input_images=trans_train['input_image'].values,\n",
    "    input_texts=trans_train['input_text'].values,\n",
    "    output_texts=trans_train['output_text'].values,\n",
    "    output_boxes=trans_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_valid_dataset = DatasetRetriever(\n",
    "    task_ids=trans_valid['task_id'].values,\n",
    "    input_images=trans_valid['input_image'].values,\n",
    "    input_texts=trans_valid['input_text'].values,\n",
    "    output_texts=trans_valid['output_text'].values,\n",
    "    output_boxes=trans_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_test_dataset = DatasetRetriever(\n",
    "    task_ids=trans_test['task_id'].values,\n",
    "    input_images=trans_test['input_image'].values,\n",
    "    input_texts=trans_test['input_text'].values,\n",
    "    output_texts=trans_test['output_text'].values,\n",
    "    output_boxes=trans_test['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='test',\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "\n",
    "#model.freeze_gpt()\n",
    "model.freeze_gpt(freeze_pos=False, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение c2c task',\n",
    "    'experiment_name': f'fusion-brain-c2c-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000005,\n",
    "    'bs': 4,\n",
    "    'num_epochs': 25,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trans_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(trans_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    trans_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(trans_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    trans_test_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(trans_test_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "        #test_loader=test_loader #before each epoch BLEU is calculated (custom_action_before_train_one_epoch in Experiment)\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac58462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @title Запуск обучения handwritten[image]\n",
    "\n",
    "DEMO_LOGS = False #@param {type:\"boolean\"}\n",
    "\n",
    "handwritten_train = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'train')]\n",
    "handwritten_valid = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'valid')]\n",
    "\n",
    "handwritten_train_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_train['task_id'].values,\n",
    "    input_images=handwritten_train['input_image'].values,\n",
    "    input_texts=handwritten_train['input_text'].values,\n",
    "    output_texts=handwritten_train['output_text'].values,\n",
    "    output_boxes=handwritten_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "handwritten_valid_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_valid['task_id'].values,\n",
    "    input_images=handwritten_valid['input_image'].values,\n",
    "    input_texts=handwritten_valid['input_text'].values,\n",
    "    output_texts=handwritten_valid['output_text'].values,\n",
    "    output_boxes=handwritten_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение handwritten task',\n",
    "    'experiment_name': f'fusion-brain-handwritten-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 32,\n",
    "    'num_epochs': 150,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(handwritten_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(handwritten_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "#model, _ = load_model(model, 'fusion-brain-c2c')\n",
    "model.freeze_gpt()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-handwritten-1631797524/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "609a9e1e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-16T16:00:22.817369\n",
      "lr: 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0, time: 261.6s, loss=3.58085, vqa_loss=3.58085\n",
      "Valid epoch 0, time: 28.2s, loss=2.82910, vqa_loss=2.82910\n",
      "\n",
      "2021-10-16T16:05:21.606019\n",
      "lr: 0.0005203251324619255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2152/2679132050.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mlast_saving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     )\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 self._print(\n",
      "\u001b[0;32m/tmp/ipykernel_2152/1437571919.py\u001b[0m in \u001b[0;36mhandle_one_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vqa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mshift_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2152/1705482069.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, task_id, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vqa'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_vqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    107\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'detection'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2152/1705482069.py\u001b[0m in \u001b[0;36mforward_vqa\u001b[0;34m(self, images, tokens, labels)\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;31m#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m             \u001b[0mtokens_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moutput_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quick-start/job_launch/FusionBrainConcept-AIJ2021/fusion_brain_utils/detection_vqa.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, text, text_mask)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0m_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_img_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4143\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muse_separate_proj_weight\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4144\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4145\u001b[0m             \u001b[0;31m# self-attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4146\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения vqa[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "vqa_train = df[(df['task_id'] == 'vqa') & (df['stage'] == 'train')]\n",
    "vqa_valid = df[(df['task_id'] == 'vqa') & (df['stage'] == 'valid')]\n",
    "\n",
    "vqa_train_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_train['task_id'].values,\n",
    "    input_images=vqa_train['input_image'].values,\n",
    "    input_texts=vqa_train['input_text'].values,\n",
    "    output_texts=vqa_train['output_text'].values,\n",
    "    output_boxes=vqa_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "vqa_valid_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model.freeze_gpt()\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение vqa task',\n",
    "    'experiment_name': f'fusion-brain-vqa-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 8,\n",
    "    'num_epochs': 20,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    vqa_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(vqa_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(vqa_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-vqa-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550c86f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Промежуточный evaluation\n",
    "vqa_valid = df[(df['task_id'] == 'vqa') & (df['stage'] == 'test')]\n",
    "\n",
    "vqa_eval_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model, metrics = load_model(model, 'fusion-brain-vqa')\n",
    "model = model.to(device)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_eval_dataset,\n",
    "    batch_size=4,\n",
    "    sampler=SequentialSampler(vqa_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=eval_fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(valid_loader, model)\n",
    "metrics[['train_vqa_loss', 'valid_vqa_loss']].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d43a46d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-16T16:07:57.863718\n",
      "lr: 4.000000000000002e-06\n",
      "Train epoch 0, time: 211.9s, loss=2.24549, loss_giou=1.08716, loss_bbox=0.76503, loss_classification=0.39330\n",
      "Valid epoch 0, time: 24.0s, loss=2.05738, loss_giou=0.96802, loss_bbox=0.70179, loss_classification=0.38757\n",
      "\n",
      "2021-10-16T16:12:02.664670\n",
      "lr: 1.80657973843171e-05\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2152/1158939347.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mlast_saving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 self._print(\n",
      "\u001b[0;32m/tmp/ipykernel_2152/1437571919.py\u001b[0m in \u001b[0;36mhandle_one_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36moptimizer_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    371\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    373\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    110\u001b[0m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m                     \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения detection[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "detection_train = df[(df['task_id'] == 'detection') & (df['stage'] == 'train')]\n",
    "detection_valid = df[(df['task_id'] == 'detection') & (df['stage'] == 'valid')]\n",
    "\n",
    "detection_train_dataset = DatasetRetriever(\n",
    "    task_ids=detection_train['task_id'].values,\n",
    "    input_images=detection_train['input_image'].values,\n",
    "    input_texts=detection_train['input_text'].values,\n",
    "    output_texts=detection_train['output_text'].values,\n",
    "    output_boxes=detection_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "detection_valid_dataset = DatasetRetriever(\n",
    "    task_ids=detection_valid['task_id'].values,\n",
    "    input_images=detection_valid['input_image'].values,\n",
    "    input_texts=detection_valid['input_text'].values,\n",
    "    output_texts=detection_valid['output_text'].values,\n",
    "    output_boxes=detection_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model.freeze_gpt()\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение detection task',\n",
    "    'experiment_name': f'fusion-brain-detection-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00001,\n",
    "    'bs': 8,\n",
    "    'num_epochs': 40,\n",
    "    'max_lr': 0.0001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    detection_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(detection_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    detection_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(detection_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=1,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02d2062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Промежуточный evaluation\n",
    "\n",
    "model, metrics = load_model(model, 'fusion-brain-detection')\n",
    "model = model.to(device)\n",
    "\n",
    "detection_eval = df[(df['task_id'] == 'detection') & (df['stage'] == 'test')]\n",
    "detection_eval_dataset = DatasetRetriever(\n",
    "    task_ids=detection_eval['task_id'].values,\n",
    "    input_images=detection_eval['input_image'].values,\n",
    "    input_texts=detection_eval['input_text'].values,\n",
    "    output_texts=detection_eval['output_text'].values,\n",
    "    output_boxes=detection_eval['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=100,\n",
    "    vqa_max_tokens_length=100,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "eval_loader = torch.utils.data.DataLoader(\n",
    "    detection_eval_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(detection_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=eval_fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(eval_loader, model)\n",
    "metrics[['train_e_acc', 'valid_e_acc']].plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403b48a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e416f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d800e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f74d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
