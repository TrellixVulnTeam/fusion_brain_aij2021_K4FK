{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU: 96\n",
      "RAM GB: 1510.6\n",
      "PyTorch version: 1.7.1+cu101\n",
      "CUDA version: 10.1\n",
      "cuDNN version: 7603\n",
      "device: cuda\n",
      "Tue Oct 26 10:46:03 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 418.181.07   Driver Version: 418.181.07   CUDA Version: 10.1     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM3...  On   | 00000000:B7:00.0 Off |                    0 |\n",
      "| N/A   32C    P0    75W / 350W |     13MiB / 32480MiB |      0%      Default |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                       GPU Memory |\n",
      "|  GPU       PID   Type   Process name                             Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# Доступные ресурсы\n",
    "import multiprocessing\n",
    "import torch\n",
    "from psutil import virtual_memory\n",
    "\n",
    "ram_gb = round(virtual_memory().total / 1024**3, 1)\n",
    "\n",
    "print('CPU:', multiprocessing.cpu_count())\n",
    "print('RAM GB:', ram_gb)\n",
    "\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"CUDA version:\", torch.version.cuda)\n",
    "print(\"cuDNN version:\", torch.backends.cudnn.version())\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device.type)\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Collecting torch==1.7.1+cu101\n",
      "  Using cached https://download.pytorch.org/whl/cu101/torch-1.7.1%2Bcu101-cp37-cp37m-linux_x86_64.whl (735.4 MB)\n",
      "Collecting torchvision==0.8.2+cu101\n",
      "  Using cached https://download.pytorch.org/whl/cu101/torchvision-0.8.2%2Bcu101-cp37-cp37m-linux_x86_64.whl (12.8 MB)\n",
      "Collecting torchaudio==0.7.2\n",
      "  Using cached torchaudio-0.7.2-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
      "Requirement already satisfied: numpy in /home/user/conda/lib/python3.7/site-packages (from torch==1.7.1+cu101) (1.18.5)\n",
      "Requirement already satisfied: typing-extensions in /home/user/conda/lib/python3.7/site-packages (from torch==1.7.1+cu101) (3.10.0.0)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/user/conda/lib/python3.7/site-packages (from torchvision==0.8.2+cu101) (8.3.1)\n",
      "Installing collected packages: torch, torchvision, torchaudio\n",
      "  Attempting uninstall: torch\n",
      "    Found existing installation: torch 1.6.0+cu101\n",
      "    Uninstalling torch-1.6.0+cu101:\n",
      "      Successfully uninstalled torch-1.6.0+cu101\n",
      "  Attempting uninstall: torchvision\n",
      "    Found existing installation: torchvision 0.7.0+cu101\n",
      "    Uninstalling torchvision-0.7.0+cu101:\n",
      "      Successfully uninstalled torchvision-0.7.0+cu101\n",
      "Successfully installed torch-1.7.1+cu101 torchaudio-0.7.2 torchvision-0.8.2+cu101\n",
      "Requirement already satisfied: tpu_star==0.0.1rc10 in /home/user/conda/lib/python3.7/site-packages (0.0.1rc10)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/conda/lib/python3.7/site-packages (from tpu_star==0.0.1rc10) (4.61.2)\n",
      "Requirement already satisfied: numpy>=1.16.5 in /home/user/conda/lib/python3.7/site-packages (from tpu_star==0.0.1rc10) (1.18.5)\n",
      "Requirement already satisfied: torch>=1.4.0 in /home/user/conda/lib/python3.7/site-packages (from tpu_star==0.0.1rc10) (1.7.1+cu101)\n",
      "Requirement already satisfied: typing-extensions in /home/user/conda/lib/python3.7/site-packages (from torch>=1.4.0->tpu_star==0.0.1rc10) (3.10.0.0)\n",
      "Requirement already satisfied: albumentations==0.5.2 in /home/user/conda/lib/python3.7/site-packages (0.5.2)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /home/user/conda/lib/python3.7/site-packages (from albumentations==0.5.2) (0.18.3)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/user/conda/lib/python3.7/site-packages (from albumentations==0.5.2) (1.18.5)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /home/user/conda/lib/python3.7/site-packages (from albumentations==0.5.2) (4.5.4.58)\n",
      "Requirement already satisfied: scipy in /home/user/conda/lib/python3.7/site-packages (from albumentations==0.5.2) (1.4.1)\n",
      "Requirement already satisfied: imgaug>=0.4.0 in /home/user/conda/lib/python3.7/site-packages (from albumentations==0.5.2) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in /home/user/conda/lib/python3.7/site-packages (from albumentations==0.5.2) (5.4.1)\n",
      "Requirement already satisfied: imageio in /home/user/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.5.2) (2.9.0)\n",
      "Requirement already satisfied: Shapely in /home/user/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.7.1)\n",
      "Requirement already satisfied: Pillow in /home/user/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.5.2) (8.3.1)\n",
      "Requirement already satisfied: six in /home/user/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.5.2) (1.16.0)\n",
      "Requirement already satisfied: opencv-python in /home/user/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.5.2) (4.5.2.54)\n",
      "Requirement already satisfied: matplotlib in /home/user/conda/lib/python3.7/site-packages (from imgaug>=0.4.0->albumentations==0.5.2) (3.4.2)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /home/user/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (1.1.1)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /home/user/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2021.10.12)\n",
      "Requirement already satisfied: networkx>=2.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations==0.5.2) (2.6.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.2) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.2) (2.4.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.2) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.5.2) (1.3.1)\n",
      "Requirement already satisfied: einops==0.3.2 in /home/user/conda/lib/python3.7/site-packages (0.3.2)\n",
      "Requirement already satisfied: transformers==4.10.0 in /home/user/conda/lib/python3.7/site-packages (4.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.0.12 in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (0.0.19)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (2021.7.6)\n",
      "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (0.10.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (4.61.2)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (3.0.12)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (1.18.5)\n",
      "Requirement already satisfied: requests in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (2.25.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (5.4.1)\n",
      "Requirement already satisfied: packaging in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (21.0)\n",
      "Requirement already satisfied: importlib-metadata in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (3.10.1)\n",
      "Requirement already satisfied: sacremoses in /home/user/conda/lib/python3.7/site-packages (from transformers==4.10.0) (0.0.46)\n",
      "Requirement already satisfied: typing-extensions in /home/user/conda/lib/python3.7/site-packages (from huggingface-hub>=0.0.12->transformers==4.10.0) (3.10.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/user/conda/lib/python3.7/site-packages (from packaging->transformers==4.10.0) (2.4.7)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata->transformers==4.10.0) (3.5.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers==4.10.0) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers==4.10.0) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers==4.10.0) (1.26.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests->transformers==4.10.0) (2021.5.30)\n",
      "Requirement already satisfied: joblib in /home/user/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.10.0) (1.0.1)\n",
      "Requirement already satisfied: six in /home/user/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.10.0) (1.16.0)\n",
      "Requirement already satisfied: click in /home/user/conda/lib/python3.7/site-packages (from sacremoses->transformers==4.10.0) (7.1.2)\n",
      "Requirement already satisfied: colorednoise==1.1.1 in /home/user/conda/lib/python3.7/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy in /home/user/conda/lib/python3.7/site-packages (from colorednoise==1.1.1) (1.18.5)\n",
      "Requirement already satisfied: catalyst==21.8 in /home/user/conda/lib/python3.7/site-packages (21.8)\n",
      "Requirement already satisfied: tensorboardX<2.3.0>=2.1.0 in /home/user/conda/lib/python3.7/site-packages (from catalyst==21.8) (1.9)\n",
      "Requirement already satisfied: hydra-slayer>=0.1.1 in /home/user/conda/lib/python3.7/site-packages (from catalyst==21.8) (0.2.0)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/user/conda/lib/python3.7/site-packages (from catalyst==21.8) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.18 in /home/user/conda/lib/python3.7/site-packages (from catalyst==21.8) (1.18.5)\n",
      "Requirement already satisfied: tqdm>=4.33.0 in /home/user/conda/lib/python3.7/site-packages (from catalyst==21.8) (4.61.2)\n",
      "Requirement already satisfied: torch>=1.3.0 in /home/user/conda/lib/python3.7/site-packages (from catalyst==21.8) (1.7.1+cu101)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/user/conda/lib/python3.7/site-packages (from tensorboardX<2.3.0>=2.1.0->catalyst==21.8) (3.17.3)\n",
      "Requirement already satisfied: six in /home/user/conda/lib/python3.7/site-packages (from tensorboardX<2.3.0>=2.1.0->catalyst==21.8) (1.16.0)\n",
      "Requirement already satisfied: typing-extensions in /home/user/conda/lib/python3.7/site-packages (from torch>=1.3.0->catalyst==21.8) (3.10.0.0)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement opencv-python==4.5.3 (from versions: 3.4.2.17, 3.4.3.18, 3.4.4.19, 3.4.5.20, 3.4.6.27, 3.4.7.28, 3.4.8.29, 3.4.9.31, 3.4.9.33, 3.4.10.35, 3.4.10.37, 3.4.11.39, 3.4.11.41, 3.4.11.43, 3.4.11.45, 3.4.13.47, 3.4.14.51, 3.4.14.53, 3.4.15.55, 3.4.16.57, 4.0.0.21, 4.0.1.23, 4.0.1.24, 4.1.0.25, 4.1.1.26, 4.1.2.30, 4.2.0.32, 4.2.0.34, 4.3.0.36, 4.3.0.38, 4.4.0.40, 4.4.0.42, 4.4.0.44, 4.4.0.46, 4.5.1.48, 4.5.2.52, 4.5.2.54, 4.5.3.56, 4.5.4.58)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for opencv-python==4.5.3\u001b[0m\n",
      "Requirement already satisfied: gdown==4.0.2 in /home/user/conda/lib/python3.7/site-packages (4.0.2)\n",
      "Requirement already satisfied: six in /home/user/conda/lib/python3.7/site-packages (from gdown==4.0.2) (1.16.0)\n",
      "Requirement already satisfied: requests[socks]>=2.12.0 in /home/user/conda/lib/python3.7/site-packages (from gdown==4.0.2) (2.25.1)\n",
      "Requirement already satisfied: filelock in /home/user/conda/lib/python3.7/site-packages (from gdown==4.0.2) (3.0.12)\n",
      "Requirement already satisfied: tqdm in /home/user/conda/lib/python3.7/site-packages (from gdown==4.0.2) (4.61.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown==4.0.2) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown==4.0.2) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown==4.0.2) (2021.5.30)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown==4.0.2) (1.26.6)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /home/user/conda/lib/python3.7/site-packages (from requests[socks]>=2.12.0->gdown==4.0.2) (1.7.1)\n",
      "Requirement already satisfied: pymorphy2 in /home/user/conda/lib/python3.7/site-packages (0.9.1)\n",
      "Requirement already satisfied: dawg-python>=0.7.1 in /home/user/conda/lib/python3.7/site-packages (from pymorphy2) (0.7.2)\n",
      "Requirement already satisfied: docopt>=0.6 in /home/user/conda/lib/python3.7/site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /home/user/conda/lib/python3.7/site-packages (from pymorphy2) (2.4.417127.4579844)\n"
     ]
    }
   ],
   "source": [
    "#!git clone https://github.com/sberbank-ai/fusion_brain_aij2021.git\n",
    "!pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install tpu_star==0.0.1rc10\n",
    "!pip install albumentations==0.5.2\n",
    "!pip install einops==0.3.2 \n",
    "!pip install transformers==4.10.0 \n",
    "!pip install colorednoise==1.1.1\n",
    "!pip install catalyst==21.8 \n",
    "!pip install opencv-python==4.5.3\n",
    "!pip install gdown==4.0.2\n",
    "!pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-26 10:46:10.915771: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
      "/home/user/conda/lib/python3.7/site-packages/pymorphy2/units/base.py:70: DeprecationWarning: inspect.getargspec() is deprecated since Python 3.0, use inspect.signature() or inspect.getfullargspec()\n",
      "  args, varargs, kw, default = inspect.getargspec(cls.__init__)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import albumentations as A\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "from glob import glob\n",
    "from datetime import datetime\n",
    "from skimage import io\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from einops import rearrange\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from catalyst.data import BalanceClassSampler, DistributedSamplerWrapper\n",
    "from torch.utils.data import Dataset, DataLoader, SequentialSampler, RandomSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tpu_star.experiment import TorchGPUExperiment\n",
    "import torch.nn.functional as F\n",
    "\n",
    "#import pytorch_lightning as pl\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, 'fusion_brain_aij2021/fb_baseline')\n",
    "from fb_utils.download import download_and_extract\n",
    "from fb_utils.loss import LabelSmoothing, onehot\n",
    "from fb_utils.metrics import cer, wer, string_accuracy, acc, vqa_evaluate, detection_evaluate\n",
    "from fb_utils.handwritten import simple_detect_lang, CTCLabeling, resize_if_need, make_img_padding\n",
    "from fb_utils.c2c_eval import Beam, eval_bleu\n",
    "from fb_utils.detection_vqa import (vqa_evaluation, detection_evaluation,\n",
    "                                              CrossAttentionLayer, MLP, FeedForwardComponent,\n",
    "                                              Resnet50Backbone, DetectionCriterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1kVFvoz6jDXckEtBQRg3pkQB_qVbsAAMG\n",
      "To: /home/jovyan/quick-start/job_launch/FusionBrainConcept-AIJ2021/detection.tar.gz\n",
      "100%|██████████| 131M/131M [00:01<00:00, 100MB/s]  \n"
     ]
    }
   ],
   "source": [
    "download_and_extract('.', 'handwritten')\n",
    "download_and_extract('.', 'detection')\n",
    "download_and_extract('.', 'vqa')\n",
    "download_and_extract('.', 'c2c')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>task_id</th>\n",
       "      <th>modality</th>\n",
       "      <th>input_image</th>\n",
       "      <th>input_text</th>\n",
       "      <th>output_text</th>\n",
       "      <th>stage</th>\n",
       "      <th>output_boxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vqa</td>\n",
       "      <td>image+text</td>\n",
       "      <td>401556.jpg</td>\n",
       "      <td>Is the toilet functional?</td>\n",
       "      <td>yes</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>46721.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>to</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>40011.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>lives</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>10196.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>the</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>46547.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>,</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>10472.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>has</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>44651.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sir</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>110640.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>п</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>trans</td>\n",
       "      <td>code</td>\n",
       "      <td>NaN</td>\n",
       "      <td>import java . util . Scanner ;   public class ...</td>\n",
       "      <td>for _ in range ( int ( input ( ) ) ) : n , m =...</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>handwritten</td>\n",
       "      <td>image</td>\n",
       "      <td>15639.png</td>\n",
       "      <td>NaN</td>\n",
       "      <td>will</td>\n",
       "      <td>train</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       task_id    modality input_image  \\\n",
       "0          vqa  image+text  401556.jpg   \n",
       "1  handwritten       image   46721.png   \n",
       "2  handwritten       image   40011.png   \n",
       "3  handwritten       image   10196.png   \n",
       "4  handwritten       image   46547.png   \n",
       "5  handwritten       image   10472.png   \n",
       "6  handwritten       image   44651.png   \n",
       "7  handwritten       image  110640.png   \n",
       "8        trans        code         NaN   \n",
       "9  handwritten       image   15639.png   \n",
       "\n",
       "                                          input_text  \\\n",
       "0                          Is the toilet functional?   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "6                                                NaN   \n",
       "7                                                NaN   \n",
       "8  import java . util . Scanner ;   public class ...   \n",
       "9                                                NaN   \n",
       "\n",
       "                                         output_text  stage output_boxes  \n",
       "0                                                yes  train          NaN  \n",
       "1                                                 to  train          NaN  \n",
       "2                                              lives  train          NaN  \n",
       "3                                                the  train          NaN  \n",
       "4                                                  ,  train          NaN  \n",
       "5                                                has  train          NaN  \n",
       "6                                                Sir  train          NaN  \n",
       "7                                                  п  train          NaN  \n",
       "8  for _ in range ( int ( input ( ) ) ) : n , m =...  train          NaN  \n",
       "9                                               will  train          NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготовка данных и сбор в единый DataFrame\n",
    "# #\n",
    "# Handwritten\n",
    "# #\n",
    "json_marking = json.load(open('handwritten/train_labels.json', 'rb'))\n",
    "marking = []\n",
    "for image_name, text in json_marking.items():\n",
    "    marking.append({\n",
    "        'path': image_name,\n",
    "        'text': text,\n",
    "        'lang': simple_detect_lang(text),\n",
    "    })\n",
    "df_handwritten = pd.DataFrame(marking)\n",
    "df_handwritten['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_handwritten.index, df_handwritten['lang']))\n",
    "df_handwritten.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Detection\n",
    "# #\n",
    "json_true_zsod = json.load(open('detection/true_zsOD.json', 'rb'))\n",
    "json_true_zsod_train = {key: json_true_zsod[key] for key in list(json_true_zsod.keys())[:900]}\n",
    "marking = []\n",
    "for image_name in json_true_zsod_train:\n",
    "    marking.extend([{\n",
    "        'task_id': 'detection',\n",
    "        'path': image_name,\n",
    "        'req': request,\n",
    "        'boxes': boxes,\n",
    "        'lang': 'en'\n",
    "    } for request, boxes in json_true_zsod_train[image_name].items() if boxes])\n",
    "df_detection = pd.DataFrame(marking)\n",
    "df_detection['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_detection.index, df_detection['lang']))\n",
    "df_detection.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# Eval Detection #\n",
    "# #\n",
    "marking = []\n",
    "json_true_zsod_test = {key: json_true_zsod[key] for key in list(json_true_zsod.keys())[900:]}\n",
    "for image_name in json_true_zsod_test:\n",
    "    marking.append({\n",
    "        'task_id': 'detection',\n",
    "        'path': image_name,\n",
    "        'req': ';'.join([request for request in json_true_zsod_test[image_name].keys()]),\n",
    "        'boxes': [boxes for boxes in json_true_zsod_test[image_name].values()],\n",
    "        'lang': 'en'\n",
    "    })\n",
    "df_eval_detection = pd.DataFrame(marking)\n",
    "df_eval_detection['stage'] = 'test'\n",
    "# #\n",
    "# VQA\n",
    "# #\n",
    "json_questions = json.load(open('vqa/questions.json', 'rb'))\n",
    "json_true_vqa = json.load(open('vqa/true_VQA.json', 'rb'))\n",
    "marking = []\n",
    "for key in json_questions:\n",
    "    if json_true_vqa[key]['lang'] == 'en':\n",
    "        marking.append({\n",
    "            'path': str(json_questions[key]['image_id']) + '.jpg',\n",
    "            'question': json_questions[key]['question'],\n",
    "            'answer': json_true_vqa[key]['answer'],\n",
    "            'lang': json_true_vqa[key]['lang']\n",
    "        })\n",
    "df_vqa = pd.DataFrame(marking)\n",
    "df_vqa['stage'] = 'train'\n",
    "skf = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)\n",
    "train_index, valid_index = next(skf.split(df_vqa.index, df_vqa['lang']))\n",
    "df_vqa.loc[valid_index, 'stage'] = 'valid'\n",
    "# #\n",
    "# C2C\n",
    "# #\n",
    "df_c2c = pd.read_json(path_or_buf='c2c/java-python.jsonl', lines=True)\n",
    "train, test = train_test_split(df_c2c, test_size=0.2)\n",
    "valid, test = train_test_split(test, test_size=0.05)\n",
    "\n",
    "df_c2c.loc[train.index.to_list(), 'stage'] = 'train'\n",
    "df_c2c.loc[valid.index.to_list(), 'stage'] = 'valid'\n",
    "df_c2c.loc[test.index.to_list(), 'stage'] = 'test'\n",
    "\n",
    "\n",
    "# #\n",
    "# Merge in common set\n",
    "# #\n",
    "dataset = []\n",
    "for image_name, text, stage in zip(df_handwritten['path'], df_handwritten['text'], df_handwritten['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'handwritten',   \n",
    "        'modality': 'image', \n",
    "        'input_image': image_name,\n",
    "        'output_text': text,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for java, python, stage in zip(df_c2c['java'], df_c2c['python'], df_c2c['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'trans',\n",
    "        'modality': 'code',    \n",
    "        'input_text': java,\n",
    "        'output_text': python,\n",
    "        'stage': stage,\n",
    "    })\n",
    "    \n",
    "for image_name, text_input, text_output, stage in zip(df_vqa['path'], df_vqa['question'], df_vqa['answer'], df_vqa['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'vqa', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_text': text_output,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for image_name, text_input, boxes, stage in zip(df_detection['path'], df_detection['req'], df_detection['boxes'], df_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "for image_name, text_input, boxes, stage in zip(df_eval_detection['path'], df_eval_detection['req'], df_eval_detection['boxes'], df_eval_detection['stage']):\n",
    "    dataset.append({\n",
    "        'task_id': 'detection', \n",
    "        'modality': 'image+text', \n",
    "        'input_image': image_name,\n",
    "        'input_text': text_input,\n",
    "        'output_boxes': boxes,\n",
    "        'stage': stage,\n",
    "    })\n",
    "\n",
    "random.shuffle(dataset)\n",
    "df = pd.DataFrame(dataset)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['modality'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DatasetRetriever(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 task_ids, \n",
    "                 input_images,\n",
    "                 input_texts,\n",
    "                 output_texts,\n",
    "                 output_boxes,\n",
    "                 ctc_labeling, \n",
    "                 tokenizer, \n",
    "                 stage, \n",
    "                 max_request_tokens_length,\n",
    "                 vqa_max_tokens_length, \n",
    "                 task_augs=None):\n",
    "        super().__init__()\n",
    "        self.task_ids = task_ids\n",
    "        \n",
    "        self.input_images = input_images\n",
    "        self.input_texts = input_texts\n",
    "        self.output_texts = output_texts\n",
    "        \n",
    "        self.task_augs = task_augs or {}\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stage = stage\n",
    "\n",
    "        # handwritten[image]:\n",
    "        self.ctc_labeling = ctc_labeling\n",
    "        self.handwritten_image_w = 512\n",
    "        self.handwritten_image_h = 128\n",
    "\n",
    "        # code2code\n",
    "        self.code_max_length = 512\n",
    "        \n",
    "        # detection[image, text]:\n",
    "        self.max_request_tokens_length = max_request_tokens_length\n",
    "        self.output_boxes = output_boxes\n",
    "        \n",
    "        # vqa[image, text]:\n",
    "        self.vqa_max_tokens_length = vqa_max_tokens_length\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        task_id = self.task_ids[idx]\n",
    "        if task_id == 'handwritten':\n",
    "            return self.get_handwritten_sample(idx)\n",
    "        elif task_id == 'trans':\n",
    "            return self.get_trans_sample(idx)\n",
    "        elif task_id == 'detection':\n",
    "            return self.get_detection_sample(idx)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.get_vqa_sample(idx)\n",
    "        return {'task_id': task_id}\n",
    "\n",
    "    def get_trans_sample(self, idx):\n",
    "            \n",
    "        source = self.input_texts[idx]\n",
    "        encoded_source = self.tokenizer.encode(str(source))\n",
    "        target = self.output_texts[idx]\n",
    "        encoded_target = self.tokenizer.encode(str(target))\n",
    "        \n",
    "        input_ids, input_labels = self.pad_and_get_mask(encoded_target, encoded_source, self.tokenizer)\n",
    "        input_ids, input_labels = torch.tensor(input_ids), torch.tensor(input_labels)\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'input_ids': input_ids,\n",
    "            'input_labels': input_labels,\n",
    "            'target': target\n",
    "        }\n",
    "\n",
    "    def get_handwritten_sample(self, idx):\n",
    "        path = 'handwritten/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image, _ = self.resize_image(image)\n",
    "\n",
    "        gt_text = self.output_texts[idx]\n",
    "        encoded = self.ctc_labeling.encode(gt_text)\n",
    "\n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('handwritten')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        ##########\n",
    "\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "\n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image': image,\n",
    "            'gt_text': gt_text,\n",
    "            'encoded': torch.tensor(encoded, dtype=torch.int32),\n",
    "        }\n",
    "    \n",
    "    def get_detection_sample(self, idx):\n",
    "        path = 'detection/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image_h, image_w, _ = image.shape\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('detection')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        image_name = self.input_images[idx]\n",
    "        ##########\n",
    "        \n",
    "        ## Input tokens ##\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_text = self.input_texts[idx]\n",
    "            input_tokens = self.tokenizer.encode_plus(input_text)\n",
    "            input_tokens['input_ids'] = input_tokens['input_ids'][:21]\n",
    "            input_tokens['attention_mask'] = input_tokens['attention_mask'][:21]\n",
    "            pad_len = self.max_request_tokens_length - len(input_tokens['input_ids'])\n",
    "            input_tokens['input_ids'] += [self.tokenizer.pad_token_id] * pad_len\n",
    "            input_tokens['attention_mask'] += [0] * pad_len\n",
    "            input_ids = torch.tensor(input_tokens['input_ids'])\n",
    "            attention_mask = torch.tensor(input_tokens['attention_mask'])\n",
    "        else:\n",
    "            input_texts = self.input_texts[idx].split(';')\n",
    "            input_ids = list(map(self.tokenizer.encode, input_texts))\n",
    "            attention_mask = [[1 for _ in input_token] for input_token in input_ids]\n",
    "            input_ids = [torch.tensor(input_id) for input_id in input_ids]\n",
    "            attention_mask = [torch.tensor(mask) for mask in attention_mask]\n",
    "        ###########\n",
    "        \n",
    "        ## Boxes ##\n",
    "        output_boxes = self.output_boxes[idx]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            output_boxes = torch.tensor(output_boxes, dtype=torch.float32)\n",
    "            output_boxes[:, 0] /= image_w\n",
    "            output_boxes[:, 1] /= image_h\n",
    "            output_boxes[:, 2] /= image_w\n",
    "            output_boxes[:, 3] /= image_h\n",
    "        else:\n",
    "            output_boxes = {\n",
    "                input_text: boxes for input_text, boxes in zip(input_texts, output_boxes)\n",
    "            }\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'boxes': output_boxes,\n",
    "            'size': (image_h, image_w)\n",
    "        }\n",
    "    \n",
    "    def get_vqa_sample(self, idx): \n",
    "        path = 'vqa/images/' + self.input_images[idx]\n",
    "        image = cv2.imread(path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        ## Augs ##\n",
    "        transforms = self.task_augs.get('vqa')\n",
    "        if transforms:\n",
    "            image = transforms(image=image)['image']\n",
    "        image = image.astype(np.float32) / 255.0\n",
    "        image = torch.from_numpy(image).permute(2, 0, 1)\n",
    "        image_name = self.input_images[idx]\n",
    "        ##########\n",
    "        \n",
    "        ## Question and Answer ##\n",
    "        input_text = self.input_texts[idx]\n",
    "        input_tokens = self.tokenizer.encode(input_text)\n",
    "        output_text = self.output_texts[idx]\n",
    "        output_tokens = self.tokenizer.encode(output_text)\n",
    "        \n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            input_tokens, output_tokens = input_tokens[:12], output_tokens[:7]\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id] + output_tokens + [self.tokenizer.eos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2] * (len(output_tokens) + 1) + [0]\n",
    "            \n",
    "            pad_len = self.vqa_max_tokens_length - len(input_ids)\n",
    "            input_ids += [self.tokenizer.pad_token_id] * pad_len\n",
    "            labels += [0] * pad_len\n",
    "        else:\n",
    "            input_ids = input_tokens + [self.tokenizer.bos_token_id]\n",
    "            labels = [1] * len(input_tokens) + [2]\n",
    "        ##########\n",
    "        \n",
    "        return {\n",
    "            'task_id': self.task_ids[idx],\n",
    "            'image_name': image_name,\n",
    "            'image': image,\n",
    "            'input_ids': torch.tensor(input_ids),\n",
    "            'labels': torch.tensor(labels),\n",
    "            'target': output_text\n",
    "        }\n",
    "\n",
    "\n",
    "    def resize_image(self, image):\n",
    "        image, coef = resize_if_need(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        image = make_img_padding(image, self.handwritten_image_h, self.handwritten_image_w)\n",
    "        return image, coef\n",
    "    \n",
    "    def pad_and_get_mask(self, target, source, tokenizer):\n",
    "        if self.stage == 'test':\n",
    "            target = []\n",
    "        while len(target) + len(source) + 2 > self.code_max_length:\n",
    "            if len(target) > len(source):\n",
    "                target = target[:-1]\n",
    "            else:\n",
    "                source = source[:-1]\n",
    "        if self.stage == 'train' or self.stage == 'valid':\n",
    "            inputs = source + [tokenizer.bos_token_id] + target + [tokenizer.eos_token_id]\n",
    "            labels = [1] * len(source) + [2] * (len(target) + 1) + [0]\n",
    "\n",
    "        else:\n",
    "            inputs = source + [tokenizer.bos_token_id]\n",
    "            labels = [1] * len(source) + [2]\n",
    "            \n",
    "            return inputs, labels\n",
    "        \n",
    "        assert len(inputs) <= self.code_max_length\n",
    "        pad_len = self.code_max_length - len(inputs)\n",
    "        inputs += [tokenizer.pad_token_id] * pad_len\n",
    "        labels += [0] * pad_len\n",
    "        assert len(inputs) == len(labels)\n",
    "        \n",
    "        return inputs, labels\n",
    "\n",
    " \n",
    "    def __len__(self) -> int:\n",
    "        return self.task_ids.shape[0]\n",
    "\n",
    "    def get_task_labels(self):\n",
    "        return list(self.task_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/albumentations/augmentations/transforms.py:1748: DeprecationWarning: This class has been deprecated. Please use ImageCompression\n",
      "  warnings.warn(\"This class has been deprecated. Please use ImageCompression\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "task_augs = {\n",
    "    'handwritten': A.Compose([\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=0.25, always_apply=False),\n",
    "        A.Rotate(limit=3, interpolation=1, border_mode=0, p=0.5),\n",
    "        A.JpegCompression(quality_lower=75, p=0.5),\n",
    "    ], p=1.0),\n",
    "    'vqa': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0),\n",
    "    'detection': A.Compose([\n",
    "        A.Resize(224, 224, always_apply=True),\n",
    "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ], p=1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df[df['stage'] == 'train']\n",
    "df_valid = df[df['stage'] == 'valid']\n",
    "df_eval = df[df['stage'] == 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Embedding(50261, 768)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Подготовка предобученной модели и токенизатора, а также CTC Labeling для задачи распознавания рукописного текста\n",
    "CHARS = ' !\"#&\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPQRSTUVWXYZ' + \\\n",
    "    '[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№'\n",
    "ctc_labeling = CTCLabeling(CHARS)\n",
    "gpt_tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "\n",
    "gpt_model = GPT2Model.from_pretrained('gpt2')\n",
    "gpt_model.resize_token_embeddings(len(gpt_tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DatasetRetriever(\n",
    "    task_ids=df_train['task_id'].values,\n",
    "    input_images=df_train['input_image'].values,\n",
    "    input_texts=df_train['input_text'].values,\n",
    "    output_texts=df_train['output_text'].values,\n",
    "    output_boxes=df_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "valid_dataset = DatasetRetriever(\n",
    "    task_ids=df_valid['task_id'].values,\n",
    "    input_images=df_valid['input_image'].values,\n",
    "    input_texts=df_valid['input_text'].values,\n",
    "    output_texts=df_valid['output_text'].values,\n",
    "    output_boxes=df_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "eval_dataset = DatasetRetriever(\n",
    "    task_ids=df_eval['task_id'].values,\n",
    "    input_images=df_eval['input_image'].values,\n",
    "    input_texts=df_eval['input_text'].values,\n",
    "    output_texts=df_eval['output_text'].values,\n",
    "    output_boxes=df_eval['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Как выглядят семплы для каждой задачи\n",
    "def demo_sample(sample):\n",
    "    if sample['task_id'] == 'handwritten':\n",
    "        print('[gt_text]:',sample['gt_text'])\n",
    "        return io.imshow(sample['image'].permute(1,2,0).numpy())\n",
    "    elif sample['task_id'] == 'trans':\n",
    "        print('[source_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[target_text]:', sample['target'])\n",
    "        return\n",
    "    elif sample['task_id'] == 'detection':\n",
    "        print('[input_text]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        print('[boxes]:', sample['boxes'].numpy())\n",
    "        return\n",
    "    elif sample['task_id'] == 'vqa':\n",
    "        print('[question and answer]:', gpt_tokenizer.decode(sample['input_ids'].numpy(), skip_special_tokens=True))\n",
    "        return\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[input_text]: a crowd of people on the beach\n",
      "[boxes]: [[0.07617188 0.2517361  0.9140625  0.19791667]]\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'detection').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[source_text]: import java. util. Scanner ;   public class Main { public static void main ( String [ ] args ) { Scanner read = new Scanner ( System. in ) ; String username = read. nextLine ( ) ; String distinctCharacters = \" \" ; for ( int i = 0 ; i < username. length ( ) ; i ++ ) { char current = username. charAt ( i ) ; if ( distinctCharacters. indexOf ( current ) < 0 ) distinctCharacters += current ; } if ( distinctCharacters. length ( ) % 2 == 0 ) { System. out. println ( \" CHAT ▁ WITH ▁ HER! \" ) ; } else { System. out. println ( \" IGNORE ▁ HIM! \" ) ; } read. close ( ) ; } }\n",
      "print ( [ \" CHAT ▁ WITH ▁ HER! \", \" IGNORE ▁ HIM! \" ] [ len ( { * input ( ) } ) % 2 ] ) NEW_LINE\n",
      "\n",
      "[target_text]: print ( [ \" CHAT ▁ WITH ▁ HER ! \" , \" IGNORE ▁ HIM ! \" ] [ len ( { * input ( ) } ) % 2 ] ) NEW_LINE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'trans').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[question and answer]: What does the bear have on his face?glasses\n"
     ]
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'vqa').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[gt_text]: transplants\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f564fd36f50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "demo_sample(train_dataset[(train_dataset.task_ids == 'handwritten').argmax()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb_collate_fn(batch):\n",
    "    \"\"\" fusion brain collate fn \"\"\"\n",
    "    encoded, encoded_length, htr_images, gt_texts = [], [], [], [] # handwritten[image]\n",
    "    code_input_ids, code_input_labels, code_targets = [], [], [] #code\n",
    "    vqa_images, vqa_input_ids, labels, targets = [], [], [], []  # vqa[image, text]\n",
    "    detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size = [], [], [], [], [], [] # detection[image, text]\n",
    "    \n",
    "    for i, sample in enumerate(batch):\n",
    "        if sample['task_id'] == 'handwritten':\n",
    "            encoded.append(sample['encoded'])\n",
    "            encoded_length.append(sample['encoded'].shape[0])\n",
    "            htr_images.append(sample['image'])\n",
    "            gt_texts.append(sample['gt_text'])\n",
    "        elif sample['task_id'] == 'trans':\n",
    "            code_input_ids.append(sample['input_ids'])\n",
    "            code_input_labels.append(sample['input_labels'])\n",
    "            code_targets.append(sample['target'])\n",
    "        elif sample['task_id'] == 'detection':\n",
    "            detection_images.append(sample['image'])\n",
    "            detection_input_ids.append(sample['input_ids'])\n",
    "            detection_attention_masks.append(sample['attention_mask'])\n",
    "            boxes.append(sample['boxes'])\n",
    "            size.append(sample['size'])\n",
    "            detection_names.append(sample['image_name'])\n",
    "        elif sample['task_id'] == 'vqa':\n",
    "            vqa_images.append(sample['image'])\n",
    "            vqa_input_ids.append(sample['input_ids'])\n",
    "            labels.append(sample['labels'])\n",
    "            targets.append(sample['target'])\n",
    "        \n",
    "    if htr_images:\n",
    "        htr_images = pad_sequence(htr_images, batch_first=True)\n",
    "        encoded, encoded_length = pad_sequence(encoded, batch_first=True), torch.tensor(encoded_length)\n",
    "    if detection_images:\n",
    "        detection_images = torch.stack(detection_images)   \n",
    "    if vqa_images:\n",
    "        vqa_images = torch.stack(vqa_images) \n",
    "    if detection_attention_masks and torch.is_tensor(detection_attention_masks[0]):\n",
    "        detection_input_ids = pad_sequence(detection_input_ids, batch_first=True)\n",
    "        detection_attention_masks = torch.stack(detection_attention_masks)\n",
    "    elif detection_attention_masks:\n",
    "        detection_input_ids = [input_id.unsqueeze(0) for input_id in detection_input_ids[0]]\n",
    "        detection_attention_masks = [attention_mask.unsqueeze(0) for attention_mask in detection_attention_masks[0]]\n",
    "    if labels:\n",
    "        vqa_input_ids = pad_sequence(vqa_input_ids, batch_first=True)\n",
    "        labels = pad_sequence(labels, batch_first=True)    \n",
    "    if code_input_ids:\n",
    "        code_input_ids = pad_sequence(code_input_ids, batch_first=True)\n",
    "        code_input_labels = pad_sequence(code_input_labels, batch_first=True)\n",
    "    return (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT2FusionBrain(nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 gpt_model, \n",
    "                 attention_config, \n",
    "                 handwritten_config, \n",
    "                 vqa_config, \n",
    "                 detection_config, \n",
    "                 **freeze_gpt_kwargs):\n",
    "        super().__init__()\n",
    "        self.gpt_model = gpt_model\n",
    "        self.embedding_size = self.gpt_model.config.n_embd\n",
    "        self.freeze_gpt(**freeze_gpt_kwargs)\n",
    "\n",
    "        # handwritten[image] input/output layers:\n",
    "        self.handwritten_config = handwritten_config\n",
    "        self.handwritten_input_layer = self._build_input_net(\n",
    "            input_dim=handwritten_config['patch_w']*handwritten_config['patch_h']*3,\n",
    "            in_layer_sizes=handwritten_config['in_layer_sizes'],\n",
    "            orth_gain=handwritten_config['orth_gain'],\n",
    "            dropout=handwritten_config['dropout'],\n",
    "        )\n",
    "        self.handwritten_lstm = nn.LSTM(\n",
    "            self.embedding_size, self.embedding_size // 2,\n",
    "            handwritten_config['lstm_num_layers'], dropout=handwritten_config['dropout'],\n",
    "            batch_first=True, bidirectional=True\n",
    "        )\n",
    "        self.handwritten_output_layer = self._build_output_net(\n",
    "            output_dim=handwritten_config['output_dim'],\n",
    "            out_layer_sizes=handwritten_config['out_layer_sizes'],\n",
    "            dropout=handwritten_config['dropout'],\n",
    "        )\n",
    "        print('=== HANDWRITTEN TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.handwritten_input_layer,\n",
    "            self.gpt_model, \n",
    "            self.handwritten_lstm, \n",
    "            self.handwritten_output_layer,\n",
    "        ], without_emb=True)\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "\n",
    "        # code2code\n",
    "        self.beam_size=3\n",
    "        self.sos_id=self.gpt_model.config.bos_token_id\n",
    "        self.eos_id=self.gpt_model.config.eos_token_id\n",
    "        self.lm_head = nn.Linear(self.gpt_model.config.n_embd, self.gpt_model.config.vocab_size, bias=False)\n",
    "\n",
    "        print('=== C2C TASK ===')\n",
    "        self._calculate_trainable_params([self.gpt_model, self.lm_head])\n",
    "        print('=== === === === ===')\n",
    "        \n",
    "        ## zhOD[image, text] and VQA[image, text] layers:\n",
    "        self.attention_config = attention_config\n",
    "        self.backbone = ResnetBackbone(pretrained=True)\n",
    "        self.input_proj = nn.Conv2d(self.backbone.num_channels, self.embedding_size, kernel_size=1)\n",
    "        self.cross_attention = nn.ModuleList([\n",
    "            CrossAttentionLayer(self.embedding_size, attention_config['num_heads'], attention_config['pf_dim'])\n",
    "            for _ in range(attention_config['num_attention_layers'])\n",
    "        ])\n",
    "        #####\n",
    "        \n",
    "        # detection[image, text] input/output layers:\n",
    "        self.detection_config = detection_config\n",
    "        self.detection_pool = nn.AdaptiveMaxPool2d((detection_config[\"num_queries\"], None))\n",
    "        self.bbox_embed = MLP(self.embedding_size, self.embedding_size, 5, detection_config['num_mlp_layers'])\n",
    "        print('=== DETECTION TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.backbone,\n",
    "            self.gpt_model, \n",
    "            self.input_proj,\n",
    "            self.cross_attention,\n",
    "            self.bbox_embed\n",
    "        ])\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "         # vqa[image, text] input/output layers:\n",
    "        self.vqa_config = vqa_config\n",
    "        self.tokens_embed = nn.Linear(self.embedding_size, vqa_config['tokens_num'])\n",
    "        print('=== VQA TASK ===')\n",
    "        self._calculate_trainable_params([\n",
    "            self.backbone,\n",
    "            self.gpt_model, \n",
    "            self.input_proj,\n",
    "            self.cross_attention,\n",
    "            self.bbox_embed\n",
    "        ])\n",
    "        print('=== === === === ===')\n",
    "        #####\n",
    "        \n",
    "        print('=== COMMON PARAMS ===')\n",
    "        self._calculate_common_params()\n",
    "        print('=== === === === ===')\n",
    "\n",
    "\n",
    "    def forward(self, task_id, **kwargs):\n",
    "        if task_id == 'handwritten':\n",
    "            return self.forward_handwritten(**kwargs)\n",
    "        elif task_id == 'trans':\n",
    "            return self.forward_trans(**kwargs)\n",
    "        elif task_id == 'vqa':\n",
    "            return self.forward_vqa(**kwargs)\n",
    "        elif task_id == 'detection':\n",
    "            return self.forward_detection(**kwargs)\n",
    "\n",
    "    def forward_trans(self, input_ids, input_labels=None, eval_bleu=False, past=None):\n",
    "        if not eval_bleu:\n",
    "            attn_mask = torch.tensor(input_labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "            attn_mask = attn_mask.to(input_labels.device)\n",
    "            outputs = self.gpt_model(input_ids, attention_mask=attn_mask)\n",
    "            x = self.lm_head(outputs[0])\n",
    "            return x\n",
    "        else:\n",
    "            if past != None:\n",
    "                outputs = self.gpt_model(input_ids, past_key_values=past)\n",
    "                logits = self.lm_head(outputs[0])\n",
    "                return logits, outputs[1]\n",
    "            else:\n",
    "                outputs = self.gpt_model(input_ids)[1]\n",
    "                return outputs\n",
    "\n",
    "    def forward_handwritten(self, images):\n",
    "        x = rearrange(images, 'b c (h p1) (w p2) -> b (w) (h) (p1 p2 c)',\n",
    "                      p1=self.handwritten_config['patch_h'], p2=self.handwritten_config['patch_w'])\n",
    "        x = x.squeeze(2)\n",
    "        x = self.handwritten_input_layer(x)\n",
    "        # Fusion Brain\n",
    "        transformer_outputs = self.gpt_model(inputs_embeds=x, output_hidden_states=True)\n",
    "        x = transformer_outputs.last_hidden_state\n",
    "        #####\n",
    "        x, _ = self.handwritten_lstm(x)        \n",
    "        x = self.handwritten_output_layer(x)\n",
    "        return x\n",
    "\n",
    "    def forward_vqa(self, images, tokens, labels):\n",
    "        back_out = self.backbone(images)\n",
    "        patchs = self.input_proj(back_out).flatten(-2).transpose(-1, -2)\n",
    "        attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "        attention_mask = attention_mask.to(labels.device)\n",
    "        # Fusion Brain\n",
    "        img_emb = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_emb = self.gpt_model(input_ids=tokens, attention_mask=attention_mask).last_hidden_state\n",
    "        #####\n",
    "        for layer in self.cross_attention:\n",
    "            tokens_emb, _ = layer(tokens_emb, img_emb)\n",
    "            \n",
    "        output_logits = self.tokens_embed(tokens_emb)\n",
    "        \n",
    "        return output_logits\n",
    "    \n",
    "    def forward_detection(self, images, tokens, attention_masks):\n",
    "        back_out = self.backbone(images)\n",
    "        patchs = self.input_proj(back_out).flatten(-2).transpose(-1, -2)\n",
    "        # Fusion Brain\n",
    "        img_embs = self.gpt_model(inputs_embeds=patchs).last_hidden_state\n",
    "        tokens_embs = self.gpt_model(input_ids=tokens, attention_mask=attention_masks).last_hidden_state\n",
    "        #####\n",
    "        norm_img_embs = F.normalize(img_embs, p=2, dim=-1)\n",
    "        norm_tokens_embs = F.normalize(tokens_embs, p=2, dim=-1)\n",
    "        \n",
    "        text_masks = attention_masks.type(torch.bool)\n",
    "        for layer in self.cross_attention:\n",
    "            img_embs, _ = layer(img_embs, tokens_embs, ~text_masks)\n",
    "        img_embs = self.detection_pool(img_embs)\n",
    "        \n",
    "        output_logits = self.bbox_embed(img_embs).sigmoid()\n",
    "        out = {\n",
    "            'pred_logits': output_logits,\n",
    "            'proj_queries': norm_img_embs,\n",
    "            'proj_tokens':norm_tokens_embs,\n",
    "        }\n",
    "\n",
    "        return out\n",
    "\n",
    "    def freeze_gpt(self, freeze_pos=True, freeze_ln=True, freeze_attn=True, freeze_ff=True, freeze_other=True):\n",
    "        for name, p in self.gpt_model.named_parameters():\n",
    "            name = name.lower()\n",
    "            if 'ln' in name or 'norm' in name:\n",
    "                p.requires_grad = not freeze_ln\n",
    "            elif 'wpe' in name or 'position_embeddings' in name or 'pos_drop' in name:\n",
    "                p.requires_grad = not freeze_pos\n",
    "            elif 'mlp' in name:\n",
    "                p.requires_grad = not freeze_ff\n",
    "            elif 'attn' in name:\n",
    "                p.requires_grad = not freeze_attn\n",
    "            else:\n",
    "                p.requires_grad = not freeze_other\n",
    "\n",
    "    def _build_input_net(self, input_dim, in_layer_sizes=None, orth_gain=1.41, dropout=0.1):\n",
    "        \"\"\" вспомогательный метод для сборки input слоя, который приводит размер входящих данный к эмбеддингу gpt \"\"\"\n",
    "        in_layer_sizes = [] if not in_layer_sizes else in_layer_sizes\n",
    "        in_layers = []\n",
    "        last_output_size = input_dim\n",
    "        for size in in_layer_sizes:\n",
    "            layer = nn.Linear(last_output_size, size)\n",
    "            if orth_gain is not None:\n",
    "                torch.nn.init.orthogonal_(layer.weight, gain=orth_gain)\n",
    "            layer.bias.data.zero_()\n",
    "\n",
    "            in_layers.append(layer)\n",
    "            in_layers.append(nn.ReLU())\n",
    "            in_layers.append(nn.Dropout(dropout))\n",
    "            last_output_size = size\n",
    "\n",
    "        final_linear = nn.Linear(last_output_size, self.embedding_size)\n",
    "        if orth_gain is not None:\n",
    "            torch.nn.init.orthogonal_(final_linear.weight, gain=orth_gain)\n",
    "        final_linear.bias.data.zero_()\n",
    "\n",
    "        in_layers.append(final_linear)\n",
    "        in_layers.append(nn.Dropout(dropout))\n",
    "\n",
    "        return nn.Sequential(*in_layers)\n",
    "    \n",
    "    def _build_output_net(self, output_dim, embedding_size=None, out_layer_sizes=None, dropout=0.1):\n",
    "        \"\"\" вспомогательный метод для сборки output слоя \"\"\"\n",
    "        out_layer_sizes = [] if not out_layer_sizes else out_layer_sizes\n",
    "        out_layers = []\n",
    "        last_output_size = embedding_size or self.embedding_size\n",
    "        for size in out_layer_sizes:\n",
    "            out_layers.append(nn.Linear(last_output_size, size))\n",
    "            out_layers.append(nn.ReLU())\n",
    "            out_layers.append(nn.Dropout(dropout))\n",
    "            last_output_size = size\n",
    "        out_layers.append(nn.Linear(last_output_size, output_dim))\n",
    "        return nn.Sequential(*out_layers)\n",
    "\n",
    "    def _calculate_trainable_params(self, layers, without_emb=False):\n",
    "        trainable_params, all_used_params = 0, 0\n",
    "        for layer in layers:\n",
    "            if layer == self.gpt_model and without_emb:\n",
    "                layer_parameters = list(layer.parameters())[2:]\n",
    "            else:\n",
    "                layer_parameters = list(layer.parameters())\n",
    "            trainable_params += sum(p.numel() for p in layer_parameters if p.requires_grad)\n",
    "            all_used_params += sum(p.numel() for p in layer_parameters)        \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print(' all_used_params:', all_used_params)\n",
    "        print('               %:', round(trainable_params/all_used_params*100, 2))\n",
    "\n",
    "    def _calculate_common_params(self):\n",
    "        all_params = sum(p.numel() for p in self.parameters())\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        common_params = sum(p.numel() for p in list(self.gpt_model.parameters())[2:])\n",
    "        print('common_params:', common_params)\n",
    "        print('   all_params:', all_params)\n",
    "        print('            %:', round(common_params/all_params*100, 2))     \n",
    "        print('trainable_params:', trainable_params)\n",
    "        print('               %:', round(trainable_params/all_params*100, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "handwritten_config = {\n",
    "    'patch_w': 8,\n",
    "    'patch_h': 128,\n",
    "    'in_layer_sizes': [8*128*3],\n",
    "    'out_layer_sizes': [64],\n",
    "    'orth_gain': 1.41,\n",
    "    'dropout': 0.1,\n",
    "    'lstm_num_layers': 3,\n",
    "    'output_dim': len(ctc_labeling), # 152\n",
    "}\n",
    "\n",
    "attention_config = {\n",
    "    'num_attention_layers': 3,\n",
    "    'num_heads': 8,\n",
    "    'pf_dim': 2048,\n",
    "}\n",
    "\n",
    "vqa_config = {\n",
    "    'tokens_num': len(gpt_tokenizer),\n",
    "}\n",
    "\n",
    "detection_config = {\n",
    "    'num_mlp_layers': 3,\n",
    "    'num_queries': 8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== HANDWRITTEN TASK ===\n",
      "trainable_params: 22494680\n",
      " all_used_params: 146937560\n",
      "               %: 15.31\n",
      "=== === === === ===\n",
      "=== C2C TASK ===\n",
      "trainable_params: 38600448\n",
      " all_used_params: 163043328\n",
      "               %: 23.67\n",
      "=== === === === ===\n",
      "=== DETECTION TASK ===\n",
      "trainable_params: 29810477\n",
      " all_used_params: 154253357\n",
      "               %: 19.33\n",
      "=== === === === ===\n",
      "=== VQA TASK ===\n",
      "trainable_params: 29810477\n",
      " all_used_params: 154253357\n",
      "               %: 19.33\n",
      "=== === === === ===\n",
      "=== COMMON PARAMS ===\n",
      "common_params: 124442880\n",
      "   all_params: 253999194\n",
      "            %: 48.99\n",
      "trainable_params: 129556314\n",
      "               %: 51.01\n",
      "=== === === === ===\n"
     ]
    }
   ],
   "source": [
    "model = GPT2FusionBrain(\n",
    "    gpt_model,\n",
    "    attention_config=attention_config,\n",
    "    handwritten_config=handwritten_config,\n",
    "    vqa_config=vqa_config,\n",
    "    detection_config=detection_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Скрипты для обучения и оценки качества модели\n",
    "\n",
    "\n",
    "class FusionBrainExperiment(TorchGPUExperiment):\n",
    "\n",
    "    handwritten_criterion = torch.nn.CTCLoss(zero_infinity=True)\n",
    "    ctc_labeling = ctc_labeling\n",
    "    detection_criterion = DetectionCriterion(['boxes', 'classification', 'contrastive'], 0.07)\n",
    "    detection_loss_weights = [1.0, 1.0, 1.0, 1.0]\n",
    "    vqa_criterion = nn.CrossEntropyLoss()\n",
    "    c2c_criterion = nn.CrossEntropyLoss()\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2', bos_token='<s>',\n",
    "            eos_token='</s>', pad_token='<pad>', unk_token='<|UNKNOWN|>', sep_token='<|SEP|>')\n",
    "    \n",
    "#     def custom_action_before_train_one_epoch(self, test_loader):\n",
    "#         run_evaluation(test_loader, self.model, tokenizer=self.tokenizer)\n",
    "        \n",
    "#     def _custom_action_before_train_one_epoch(self):\n",
    "#         self._wipe_memory()\n",
    "#         self.custom_action_before_train_one_epoch(test_loader)\n",
    "\n",
    "    def calculate_handwritten_metrics(self, gt_texts, outputs):\n",
    "        pred_texts = []\n",
    "        for encoded in outputs.argmax(2).data.cpu().numpy():\n",
    "            pred_texts.append(self.ctc_labeling.decode(encoded))\n",
    "        texts = [self.ctc_labeling.preprocess(text) for text in gt_texts]\n",
    "        return {\n",
    "            'h_cer': cer(pred_texts, texts),\n",
    "            'h_wer': wer(pred_texts, texts),\n",
    "            'h_acc': string_accuracy(pred_texts, texts),\n",
    "        }\n",
    "\n",
    "    def handle_one_batch(self, batch):\n",
    "        (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size) = batch\n",
    "        losses = []\n",
    "        metrics = {}\n",
    "\n",
    "        if len(htr_images) > 0:\n",
    "            bs = htr_images.shape[0]\n",
    "            images = htr_images.to(self.device, dtype=torch.float32)\n",
    "            encoded_length = encoded_length.to(self.device, dtype=torch.int32)\n",
    "            encoded = encoded.to(self.device, dtype=torch.int32)\n",
    "            handwritten_outputs = self.model('handwritten', images=images)\n",
    "            preds_size = torch.IntTensor([handwritten_outputs.size(1)] * bs)\n",
    "            preds = handwritten_outputs.log_softmax(2).permute(1, 0, 2)\n",
    "            handwritten_loss = self.handwritten_criterion(preds, encoded, preds_size, encoded_length)\n",
    "            handwritten_metrics = self.calculate_handwritten_metrics(gt_texts, handwritten_outputs)\n",
    "            metrics.update(handwritten_metrics)\n",
    "            metrics['h_loss'] = handwritten_loss.detach().cpu().item()\n",
    "            losses.append(handwritten_loss)\n",
    "\n",
    "        if len(code_input_ids) > 0:\n",
    "            bs = code_input_ids.shape[0]\n",
    "            code_input_ids = code_input_ids.to(self.device, dtype=torch.long) \n",
    "            code_input_labels = code_input_labels.to(self.device, dtype=torch.long) \n",
    "            loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('trans', input_ids=code_input_ids, input_labels=code_input_labels)\n",
    "            c_labels = code_input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = c_labels[..., 1:].contiguous()\n",
    "            \n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            c2c_loss = self.c2c_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['c2c_loss'] = c2c_loss.detach().cpu().item()\n",
    "            losses.append(c2c_loss)\n",
    "            \n",
    "        if len(labels) > 0:\n",
    "            images = vqa_images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = vqa_input_ids.to(self.device, dtype=torch.long)\n",
    "            labels = labels.to(self.device, dtype=torch.float32)\n",
    "            loss_mask = torch.tensor(labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "            loss_mask = loss_mask.to(self.device)\n",
    "            lm_logits = self.model('vqa', images=images, tokens=input_ids, labels=labels)\n",
    "            labels = input_ids\n",
    "            shift_logits = lm_logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "\n",
    "            flatten_shift_loss_mask = loss_mask[..., :-1].contiguous().view(-1)\n",
    "            ids = torch.nonzero(flatten_shift_loss_mask).view(-1)\n",
    "            vqa_loss = self.vqa_criterion(shift_logits.view(-1, shift_logits.size(-1))[ids], shift_labels.view(-1)[ids])\n",
    "            metrics['vqa_loss'] = vqa_loss.detach().cpu().item()\n",
    "            losses.append(vqa_loss)\n",
    "            \n",
    "        if len(boxes) > 0:\n",
    "            images = detection_images.to(self.device, dtype=torch.float32)\n",
    "            input_ids = detection_input_ids.to(self.device, dtype=torch.long) \n",
    "            attention_masks = detection_attention_masks.to(self.device, dtype=torch.long) \n",
    "            boxes = [boxes_per_label.to(self.device, dtype=torch.float) for boxes_per_label in boxes]\n",
    "            detection_outputs = self.model('detection', images=images, tokens=input_ids, attention_masks=attention_masks)\n",
    "            detection_loss = self.detection_criterion(detection_outputs, boxes)\n",
    "            #detection_loss = sum([\n",
    "            #    loss * weight for loss, weight in zip(detection_loss.values(), self.detection_loss_weights)\n",
    "            #])\n",
    "            #metrics['detection_acc'] = acc(targets.argmax(axis=1), sentiment_outputs)\n",
    "            metrics['loss_giou'] = detection_loss['loss_giou'].detach().cpu().item()\n",
    "            metrics['loss_bbox'] = detection_loss['loss_bbox'].detach().cpu().item()\n",
    "            metrics['loss_contrastive'] = detection_loss['loss_contrastive'].detach().cpu().item()\n",
    "            metrics['loss_classification'] = detection_loss['loss_classification'].detach().cpu().item()\n",
    "            losses.append(detection_loss['loss_giou'])\n",
    "            losses.append(detection_loss['loss_bbox'])\n",
    "            losses.append(detection_loss['loss_contrastive'])\n",
    "            losses.append(detection_loss['loss_classification'])\n",
    "\n",
    "        #loss = sum(losses)\n",
    "        loss = sum([loss * weight for loss, weight in zip(losses, self.detection_loss_weights)])\n",
    "        \n",
    "        #print(loss)\n",
    "        self.metrics.update(loss=loss.detach().cpu().item(), **metrics)\n",
    "        \n",
    "        #self.metrics.update(loss=loss.item(), **metrics)\n",
    "\n",
    "        if self.is_train:\n",
    "            loss.backward()\n",
    "            self.optimizer_step()\n",
    "            self.optimizer.zero_grad()\n",
    "            self.scheduler.step()\n",
    "\n",
    "\n",
    "def run_evaluation(loader, model, tokenizer=None, device=torch.device('cuda:0')):\n",
    "    result = []\n",
    "    true_json_detection = {}\n",
    "    pred_json_detection = {}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader):\n",
    "            (htr_images, encoded, encoded_length, gt_texts), (code_input_ids, code_input_labels, code_targets), (vqa_images, vqa_input_ids, labels, targets), (detection_names, detection_images, detection_input_ids, detection_attention_masks, boxes, size) = batch\n",
    "            if len(htr_images) > 0:\n",
    "                images = htr_images.to(device, dtype=torch.float32)\n",
    "                handwritten_outputs = model('handwritten', images=images)\n",
    "                for encoded, gt_text in zip(handwritten_outputs.argmax(2).data.cpu().numpy(), gt_texts):\n",
    "                    pred_text = ctc_labeling.decode(encoded)\n",
    "                    result.append({\n",
    "                        'task_id': 'handwritten',\n",
    "                        'gt_output': gt_text,\n",
    "                        'pred_output': pred_text,\n",
    "                    })\n",
    "\n",
    "            if len(code_input_ids) > 0:\n",
    "                code_input_ids = code_input_ids.to(device, dtype=torch.long)\n",
    "                code_input_labels = code_input_labels.to(device, dtype=torch.long)\n",
    "                loss_mask = torch.tensor(code_input_labels.clone().detach() == 2, dtype=torch.uint8)\n",
    "                loss_mask = loss_mask.to(device)\n",
    "                hidden_states = model('trans', input_ids=code_input_ids, input_labels=code_input_labels, eval_bleu=True)\n",
    "                bleu_score, _ = eval_bleu(model, hidden_states, input_ids=code_input_ids, beam_size=5, tokenizer=tokenizer, targets=code_targets)\n",
    "                result.append({\n",
    "                        'task_id': 'trans',\n",
    "                        'true_text': code_targets,\n",
    "                        'bleu_score': bleu_score,\n",
    "                })\n",
    "                \n",
    "            if len(labels) > 0:\n",
    "                images = vqa_images.to(device, dtype=torch.float32)\n",
    "                input_ids = vqa_input_ids.to(device, dtype=torch.long)\n",
    "                labels = labels.to(device, dtype=torch.float) \n",
    "                attention_mask = torch.tensor(labels.clone().detach() != 0, dtype=torch.uint8)\n",
    "                attention_mask = attention_mask.to(labels.device)\n",
    "                vqa_outputs = vqa_evaluation(model, images, input_ids, attention_mask, 10)\n",
    "                for target, pred_labels in zip(targets, vqa_outputs.argmax(-1).cpu().numpy()):\n",
    "                    result.append({\n",
    "                        'task_id': 'vqa',\n",
    "                        'gt_output': target,\n",
    "                        'pred_output': gpt_tokenizer.decode(pred_labels).split(gpt_tokenizer.eos_token)[0],\n",
    "                    })\n",
    "                    \n",
    "            if len(boxes) > 0:\n",
    "                images = detection_images.to(device, dtype=torch.float32)\n",
    "                input_ids = [input_id.to(device, dtype=torch.long) for input_id in detection_input_ids]\n",
    "                attention_masks = [attention_mask.to(device, dtype=torch.long) for attention_mask in detection_attention_masks]\n",
    "                detection_outputs = detection_evaluation(model, images, input_ids, attention_masks, 0.12, 0.5)\n",
    "                img_h, img_w = size[0]\n",
    "                for i in range(len(detection_outputs)):\n",
    "                    if detection_outputs[i].numel() != 0:\n",
    "                        detection_outputs[i][:, 0] = detection_outputs[i][:, 0] * img_w\n",
    "                        detection_outputs[i][:, 2] = detection_outputs[i][:, 2] * img_w\n",
    "                        detection_outputs[i][:, 1] = detection_outputs[i][:, 1] * img_h\n",
    "                        detection_outputs[i][:, 3] = detection_outputs[i][:, 3] * img_h\n",
    "                image_name = detection_names[0]\n",
    "                for boxes_for_img in boxes:\n",
    "                    true_json_detection[image_name] = boxes_for_img\n",
    "                    pred_json_detection[image_name] = {\n",
    "                        input_text: output.type(torch.int32).cpu().tolist()\n",
    "                        for input_text, output in zip(boxes_for_img.keys(), detection_outputs)\n",
    "                    }\n",
    "                result.append({\n",
    "                        'task_id': 'detection',\n",
    "                    })\n",
    "                \n",
    "    result = pd.DataFrame(result)\n",
    "\n",
    "    handwritten_result = result[result['task_id'] == 'handwritten']\n",
    "    if handwritten_result.shape[0]:\n",
    "        print('= Handwritten =')\n",
    "        print('CER:', round(cer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('WER:', round(wer(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('ACC:', round(string_accuracy(handwritten_result['pred_output'], handwritten_result['gt_output']), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    trans_result = result[result['task_id'] == 'trans']   \n",
    "    if trans_result.shape[0]:\n",
    "        print('== C2C ==')\n",
    "        print('meanBLEU:', np.mean(trans_result['bleu_score']))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    vqa_result = result[result['task_id'] == 'vqa']\n",
    "    if vqa_result.shape[0]:\n",
    "        print('== VQA ==')\n",
    "        print('ACC:', round(vqa_evaluate(vqa_result), 3))\n",
    "        print('=== === === ===')\n",
    "        \n",
    "    \n",
    "    if len(true_json_detection):\n",
    "        print('== Detection ==')\n",
    "        #print(true_json_detection)\n",
    "        #print(\"PRED\")\n",
    "        #print(pred_json_detection)\n",
    "        print('ACC:', round(detection_evaluate(true_json_detection, pred_json_detection), 3))\n",
    "        print('=== === === ===')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def load_model(model, experiment_name):\n",
    "    paths = sorted(glob(f'./saved_models/{experiment_name}*'))\n",
    "    if len(paths) == 0:\n",
    "        print('Warning! Model not found')\n",
    "        return model\n",
    "    checkpoint_path = paths[-1] + '/last.pt'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     metrics = defaultdict(list)\n",
    "#     for epoch in range(len(checkpoint['metrics_state_dict']['train_metrics'])):\n",
    "#         train_metrics = checkpoint['metrics_state_dict']['train_metrics'][epoch]['avg']\n",
    "#         valid_metrics = checkpoint['metrics_state_dict']['valid_metrics'][epoch]['avg']\n",
    "#         for key in train_metrics.keys():\n",
    "#             metrics[f'train_{key}'].append(train_metrics[key])\n",
    "#             metrics[f'valid_{key}'].append(valid_metrics[key])\n",
    "#         metrics['epoch'].append(epoch)        \n",
    "#     metrics = pd.DataFrame(metrics)\n",
    "    return model\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code To Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-26T08:08:34.473560\n",
      "lr: 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1873 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_537/2841111320.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m#test_loader=test_loader #before each epoch BLEU is calculated (custom_action_before_train_one_epoch in Experiment)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     )\n\u001b[0;32m--> 125\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 self._print(\n",
      "\u001b[0;32m/tmp/ipykernel_537/2434122307.py\u001b[0m in \u001b[0;36mhandle_one_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_train\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    219\u001b[0m                 \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m                 create_graph=create_graph)\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    130\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    131\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения c2c[code]\n",
    "\n",
    "DEMO_LOGS = False \n",
    "\n",
    "trans_train = df[(df['task_id'] == 'trans') & (df['stage'] == 'train')]\n",
    "trans_valid = df[(df['task_id'] == 'trans') & (df['stage'] == 'valid')]\n",
    "trans_test = df[(df['task_id'] == 'trans') & (df['stage'] == 'test')]\n",
    "\n",
    "\n",
    "trans_train_dataset = DatasetRetriever(\n",
    "    task_ids=trans_train['task_id'].values,\n",
    "    input_images=trans_train['input_image'].values,\n",
    "    input_texts=trans_train['input_text'].values,\n",
    "    output_texts=trans_train['output_text'].values,\n",
    "    output_boxes=trans_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_valid_dataset = DatasetRetriever(\n",
    "    task_ids=trans_valid['task_id'].values,\n",
    "    input_images=trans_valid['input_image'].values,\n",
    "    input_texts=trans_valid['input_text'].values,\n",
    "    output_texts=trans_valid['output_text'].values,\n",
    "    output_boxes=trans_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "trans_test_dataset = DatasetRetriever(\n",
    "    task_ids=trans_test['task_id'].values,\n",
    "    input_images=trans_test['input_image'].values,\n",
    "    input_texts=trans_test['input_text'].values,\n",
    "    output_texts=trans_test['output_text'].values,\n",
    "    output_boxes=trans_test['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='test',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "\n",
    "#model.freeze_gpt()\n",
    "model.freeze_gpt(freeze_pos=False, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение c2c task',\n",
    "    'experiment_name': f'fusion-brain-c2c-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000005,\n",
    "    'bs': 4,\n",
    "    'num_epochs': 25,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    trans_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(trans_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    trans_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(trans_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    trans_test_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(trans_test_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "        #test_loader=test_loader #before each epoch BLEU is calculated (custom_action_before_train_one_epoch in Experiment)\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 100/100 [02:16<00:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== C2C ==\n",
      "meanBLEU: 26.071800000000003\n",
      "=== === === ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# c2c evaluation\n",
    "c2c_valid = df[(df['task_id'] == 'trans') & (df['stage'] == 'test')][:100]\n",
    "\n",
    "c2c_eval_dataset = DatasetRetriever(\n",
    "    task_ids=c2c_valid['task_id'].values,\n",
    "    input_images=c2c_valid['input_image'].values,\n",
    "    input_texts=c2c_valid['input_text'].values,\n",
    "    output_texts=c2c_valid['output_text'].values,\n",
    "    output_boxes=c2c_valid['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model, _ = load_model(model, 'fusion-brain-c2c')\n",
    "model = model.to(device)\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    c2c_eval_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(c2c_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(valid_loader, model, tokenizer=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения handwritten[image]\n",
    "\n",
    "DEMO_LOGS = False #@param {type:\"boolean\"}\n",
    "\n",
    "handwritten_train = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'train')]\n",
    "handwritten_valid = df[(df['task_id'] == 'handwritten') & (df['stage'] == 'valid')]\n",
    "\n",
    "handwritten_train_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_train['task_id'].values,\n",
    "    input_images=handwritten_train['input_image'].values,\n",
    "    input_texts=handwritten_train['input_text'].values,\n",
    "    output_texts=handwritten_train['output_text'].values,\n",
    "    output_boxes=handwritten_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "handwritten_valid_dataset = DatasetRetriever(\n",
    "    task_ids=handwritten_valid['task_id'].values,\n",
    "    input_images=handwritten_valid['input_image'].values,\n",
    "    input_texts=handwritten_valid['input_text'].values,\n",
    "    output_texts=handwritten_valid['output_text'].values,\n",
    "    output_boxes=handwritten_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение handwritten task',\n",
    "    'experiment_name': f'fusion-brain-handwritten-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 50,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(handwritten_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    handwritten_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(handwritten_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "#model, _ = load_model(model, 'fusion-brain-c2c')\n",
    "model.freeze_gpt()\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-handwritten-1631797524/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=False,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-26T08:16:09.280235\n",
      "lr: 3.9999999999999996e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:73: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train epoch 0, time: 48.6s, loss=4.79440, vqa_loss=4.79440\n",
      "Valid epoch 0, time: 4.7s, loss=3.47487, vqa_loss=3.47487\n",
      "\n",
      "2021-10-26T08:17:17.206440\n",
      "lr: 0.0005226089222300143\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_537/3956598062.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlast_saving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(self, train_loader)\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m                 self._print(\n",
      "\u001b[0;32m/tmp/ipykernel_537/2434122307.py\u001b[0m in \u001b[0;36mhandle_one_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mloss_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m             \u001b[0mlm_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vqa'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mshift_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlm_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_537/4040727093.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, task_id, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'vqa'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_vqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'detection'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_537/4040727093.py\u001b[0m in \u001b[0;36mforward_vqa\u001b[0;34m(self, images, tokens, labels)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m#####\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_attention\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mtokens_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0moutput_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens_embed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens_emb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quick-start/job_launch/FusionBrainConcept-AIJ2021/fb_utils/detection_vqa.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img, text, text_mask)\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0m_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_img_attention\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtext_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mself_attn_layer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask)\u001b[0m\n\u001b[1;32m    983\u001b[0m                 \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m                 \u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkey_padding_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneed_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mneed_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m                 attn_mask=attn_mask)\n\u001b[0m\u001b[1;32m    986\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\u001b[0m\n\u001b[1;32m   4146\u001b[0m             \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_proj_bias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4148\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4149\u001b[0m             \u001b[0;31m# encoder-decoder attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4150\u001b[0m             \u001b[0;31m# This is inline in_proj function with in_proj_weight and in_proj_bias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения vqa[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "vqa_train = df[(df['task_id'] == 'vqa') & (df['stage'] == 'train')]\n",
    "vqa_valid = df[(df['task_id'] == 'vqa') & (df['stage'] == 'valid')]\n",
    "\n",
    "vqa_train_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_train['task_id'].values,\n",
    "    input_images=vqa_train['input_image'].values,\n",
    "    input_texts=vqa_train['input_text'].values,\n",
    "    output_texts=vqa_train['output_text'].values,\n",
    "    output_boxes=vqa_train['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='train',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "vqa_valid_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    stage='valid',\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение vqa task',\n",
    "    'experiment_name': f'fusion-brain-vqa-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000004,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 20,\n",
    "    'max_lr': 0.001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    vqa_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(vqa_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(vqa_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "#model, _ = load_model(model, 'fusion-brain-handwritten')\n",
    "model.freeze_gpt(freeze_pos=False, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-vqa-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/user/conda/lib/python3.7/site-packages/ipykernel_launcher.py:158: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "100%|██████████| 1000/1000 [02:18<00:00,  7.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== VQA ==\n",
      "ACC: 0.3\n",
      "=== === === ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# vqa evaluation\n",
    "vqa_valid = df[(df['task_id'] == 'vqa') & (df['stage'] == 'valid')][:1000]\n",
    "\n",
    "vqa_eval_dataset = DatasetRetriever(\n",
    "    task_ids=vqa_valid['task_id'].values,\n",
    "    input_images=vqa_valid['input_image'].values,\n",
    "    input_texts=vqa_valid['input_text'].values,\n",
    "    output_texts=vqa_valid['output_text'].values,\n",
    "    output_boxes=vqa_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('fusion.pt')['model_state_dict'])\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    vqa_eval_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(vqa_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(valid_loader, model, tokenizer=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2021-10-26T08:17:54.619767\n",
      "lr: 4.000000000000002e-06\n",
      "Train epoch 0, time: 52.8s, loss=5.94977, loss_giou=1.11608, loss_bbox=0.90062, loss_contrastive=3.46927, loss_classification=0.46380\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_537/3222094831.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0mlast_saving\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     )\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdestroy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, train_loader, valid_loader, n_epochs)\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m             \u001b[0mepoch_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rebuild_loader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m             \u001b[0;31m# #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mstage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'valid'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/tpu_star/experiment/torch_gpu.py\u001b[0m in \u001b[0;36mvalidation\u001b[0;34m(self, valid_loader)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 self._print(\n",
      "\u001b[0;32m/tmp/ipykernel_537/2434122307.py\u001b[0m in \u001b[0;36mhandle_one_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0mattention_masks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdetection_attention_masks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mboxes_per_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mboxes_per_label\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0mdetection_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'detection'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m             \u001b[0mdetection_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;31m#detection_loss = sum([\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_537/4040727093.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, task_id, **kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_vqa\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtask_id\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'detection'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_trans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meval_bleu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpast\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_537/4040727093.py\u001b[0m in \u001b[0;36mforward_detection\u001b[0;34m(self, images, tokens, attention_masks)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_detection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_masks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mback_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0mpatchs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mback_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# Fusion Brain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/quick-start/job_launch/FusionBrainConcept-AIJ2021/fb_utils/detection_vqa.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackbone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    729\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    418\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    419\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 420\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Запуск обучения detection[image+text]\n",
    "\n",
    "DEMO_LOGS = False\n",
    "\n",
    "detection_train = df[(df['task_id'] == 'detection') & (df['stage'] == 'train')]\n",
    "detection_valid = df[(df['task_id'] == 'detection') & (df['stage'] == 'valid')]\n",
    "\n",
    "detection_train_dataset = DatasetRetriever(\n",
    "    task_ids=detection_train['task_id'].values,\n",
    "    input_images=detection_train['input_image'].values,\n",
    "    input_texts=detection_train['input_text'].values,\n",
    "    output_texts=detection_train['output_text'].values,\n",
    "    output_boxes=detection_train['output_boxes'].values,\n",
    "    stage='train',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "detection_valid_dataset = DatasetRetriever(\n",
    "    task_ids=detection_valid['task_id'].values,\n",
    "    input_images=detection_valid['input_image'].values,\n",
    "    input_texts=detection_valid['input_text'].values,\n",
    "    output_texts=detection_valid['output_text'].values,\n",
    "    output_boxes=detection_valid['output_boxes'].values,\n",
    "    stage='valid',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Обучение detection task',\n",
    "    'experiment_name': f'fusion-brain-detection-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.000004,\n",
    "    'bs': 32,\n",
    "    'num_epochs': 60,\n",
    "    'max_lr': 0.0001,\n",
    "    'pct_start': 0.1,\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    detection_train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=RandomSampler(detection_train_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    detection_valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=SequentialSampler(detection_valid_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=4,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "#model, _ = load_model(model, 'fusion-brain-vqa')\n",
    "model.freeze_gpt(freeze_pos=True, freeze_ln=False, freeze_attn=True, freeze_ff=True, freeze_other=False)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-detection-1631794561/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 45/45 [00:12<00:00,  3.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Detection ==\n",
      "ACC: 0.108\n",
      "=== === === ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# detection evaluation\n",
    "detection_valid = df[(df['task_id'] == 'detection') & (df['stage'] == 'test')][:1000]\n",
    "\n",
    "detection_eval_dataset = DatasetRetriever(\n",
    "    task_ids=detection_valid['task_id'].values,\n",
    "    input_images=detection_valid['input_image'].values,\n",
    "    input_texts=detection_valid['input_text'].values,\n",
    "    output_texts=detection_valid['output_text'].values,\n",
    "    output_boxes=detection_valid['output_boxes'].values,\n",
    "    stage='test',\n",
    "    ctc_labeling=ctc_labeling,\n",
    "    tokenizer=gpt_tokenizer,\n",
    "    max_request_tokens_length=21,\n",
    "    vqa_max_tokens_length=21,\n",
    "    task_augs=task_augs,\n",
    ")\n",
    "\n",
    "model = model.to(device)\n",
    "#model.load_state_dict(torch.load('fusion.pt')['model_state_dict'])\n",
    "\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    detection_eval_dataset,\n",
    "    batch_size=1,\n",
    "    sampler=SequentialSampler(detection_eval_dataset),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "evaluation_result = run_evaluation(valid_loader, model, tokenizer=gpt_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fusion Brain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запуск обучения fusion brain [text, image]\n",
    "\n",
    "DEMO_LOGS = False #@param {type:\"boolean\"}\n",
    "\n",
    "CONFIG = {\n",
    "    'description': 'Fusion Brain',\n",
    "    'experiment_name': f'fusion-brain-main-{round(datetime.utcnow().timestamp())}',\n",
    "    'lr': 0.00008,\n",
    "    'bs': 64,\n",
    "    'num_epochs': 20,\n",
    "    'max_lr': 0.00004,\n",
    "    'pct_start': 0.1,\n",
    "    'final_div_factor': 100\n",
    "}\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=BalanceClassSampler(labels=train_dataset.get_task_labels()),\n",
    "    pin_memory=False,\n",
    "    drop_last=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "valid_loader = torch.utils.data.DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=CONFIG['bs'],\n",
    "    sampler=BalanceClassSampler(labels=valid_dataset.get_task_labels()),\n",
    "    pin_memory=False,\n",
    "    drop_last=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=fb_collate_fn,\n",
    ")\n",
    "\n",
    "#model, _ = load_model(model, 'fusion-brain-detection')\n",
    "model.freeze_gpt(False, False, False, False, False)\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CONFIG['lr'])\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=CONFIG['max_lr'],\n",
    "    steps_per_epoch=len(train_loader), \n",
    "    pct_start=CONFIG['pct_start'],\n",
    "    epochs=CONFIG['num_epochs'],\n",
    "    final_div_factor=CONFIG['final_div_factor'],\n",
    ")\n",
    "\n",
    "if DEMO_LOGS:\n",
    "    print(open('./saved_models/fusion-brain-main-1631924391/log.txt').read())\n",
    "else:\n",
    "    experiment = FusionBrainExperiment(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        criterion=None,\n",
    "        scheduler=scheduler,\n",
    "        device=device,\n",
    "        base_dir=f'./saved_models',\n",
    "        experiment_name=CONFIG['experiment_name'],\n",
    "        verbose_step=10**5,\n",
    "        seed=42,\n",
    "        best_saving=True,\n",
    "        last_saving=True,\n",
    "    )\n",
    "    experiment.fit(train_loader, valid_loader, CONFIG['num_epochs'])\n",
    "    experiment.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
